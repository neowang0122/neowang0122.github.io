---
id: 558e24ba-d655-4dfb-bc0c-4adaa5475300
title: |
  Reservoir computing - Wikipedia
author: |
  Contributors to Wikimedia projects
date_saved: 2023-11-12 11:58:12
date_published: 2007-04-14 16:34:00
draft: true
---

# Reservoir computing - Wikipedia
#Omnivore

[Read on Omnivore](https://omnivore.app/me/reservoir-computing-wikipedia-18bc4782d66)

[Read Original](https://en.wikipedia.org/wiki/Reservoir_computing)

date_saved: 2023-11-12 11:58:12

date_published: 2007-04-14 16:34:00

--- 

# Full Content: 

From Wikipedia, the free encyclopedia

**Reservoir computing** is a framework for computation derived from [recurrent neural network](https://en.wikipedia.org/wiki/Recurrent%5Fneural%5Fnetwork "Recurrent neural network") theory that maps input signals into higher dimensional computational spaces through the dynamics of a fixed, non-linear system called a reservoir. After the input signal is fed into the reservoir, which is treated as a "black box," a simple readout mechanism is trained to read the state of the reservoir and map it to the desired output. The first key benefit of this framework is that training is performed only at the readout stage, as the reservoir dynamics are fixed. The second is that the computational power of naturally available systems, both classical and quantum mechanical, can be used to reduce the effective computational cost.

## History

The concept of reservoir computing stems from the use of recursive connections within [neural networks](https://en.wikipedia.org/wiki/Neural%5Fnetwork "Neural network") to create a complex dynamical system. It is a generalisation of earlier neural network architectures such as recurrent neural networks, [liquid-state machines](https://en.wikipedia.org/wiki/Liquid%5Fstate%5Fmachine "Liquid state machine") and [echo-state networks](https://en.wikipedia.org/wiki/Echo%5Fstate%5Fnetwork "Echo state network"). Reservoir computing also extends to physical systems that are not networks in the classical sense, but rather continuous systems in space and/or time: e.g. a literal "bucket of water" can serve as a reservoir that performs computations on inputs given as perturbations of the surface. The resultant complexity of such recurrent neural networks was found to be useful in solving a variety of problems including language processing and dynamic system modeling. However, training of recurrent neural networks is challenging and computationally expensive. Reservoir computing reduces those training-related challenges by fixing the dynamics of the reservoir and only training the linear output layer.

A large variety of nonlinear dynamical systems can serve as a reservoir that performs computations. In recent years semiconductor lasers have attracted considerable interest as computation can be fast and energy efficient compared to electrical components.

Recent advances in both AI and quantum information theory have given rise to the concept of [quantum neural networks](https://en.wikipedia.org/wiki/Quantum%5Fneural%5Fnetworks "Quantum neural networks"). These hold promise in quantum information processing, which is challenging to classical networks, but can also find application in solving classical problems. In 2018, a physical realization of a quantum reservoir computing architecture was demonstrated in the form of nuclear spins within a molecular solid. However, the nuclear spin experiments in did not demonstrate quantum reservoir computing per se as they did not involve processing of sequential data. Rather the data were vector inputs, which makes this more accurately a demonstration of quantum implementation of a [random kitchen sink](https://en.wikipedia.org/w/index.php?title=Random%5Fkitchen%5Fsink&action=edit&redlink=1 "Random kitchen sink (page does not exist)") algorithm (also going by the name of [extreme learning machines](https://en.wikipedia.org/wiki/Extreme%5Flearning%5Fmachine "Extreme learning machine") in some communities). In 2019, another possible implementation of quantum reservoir processors was proposed in the form of two-dimensional fermionic lattices. In 2020, realization of reservoir computing on gate-based quantum computers was proposed and demonstrated on cloud-based IBM superconducting near-term quantum computers.

Reservoir computers have been used for [time-series](https://en.wikipedia.org/wiki/Time%5Fseries "Time series") analysis purposes. In particular, some of their usages involve [chaotic](https://en.wikipedia.org/wiki/Chaos%5Ftheory "Chaos theory") [time-series](https://en.wikipedia.org/wiki/Time%5Fseries "Time series") prediction, separation of [chaotic](https://en.wikipedia.org/wiki/Chaos%5Ftheory "Chaos theory") signals, and link inference of [networks](https://en.wikipedia.org/wiki/Network%5Ftheory "Network theory") from their dynamics.

### Reservoir

The 'reservoir' in reservoir computing is the internal structure of the computer, and must have two properties: it must be made up of individual, non-linear units, and it must be capable of storing information. The non-linearity describes the response of each unit to input, which is what allows reservoir computers to solve complex problems. Reservoirs are able to store information by connecting the units in recurrent loops, where the previous input affects the next response. The change in reaction due to the past allows the computers to be trained to complete specific tasks.

Reservoirs can be virtual or physical. Virtual reservoirs are typically randomly generated and are designed like neural networks. Virtual reservoirs can be designed to have non-linearity and recurrent loops, but, unlike neural networks, the connections between units are randomized and remain unchanged throughout computation. Physical reservoirs are possible because of the inherent non-linearity of certain natural systems. The interaction between ripples on the surface of water contains the nonlinear dynamics required in reservoir creation, and a pattern recognition RC was developed by first inputting ripples with electric motors then recording and analyzing the ripples in the readout.

### Readout

The readout is a neural network layer that performs a linear transformation on the output of the reservoir. The weights of the readout layer are trained by analyzing the spatiotemporal patterns of the reservoir after excitation by known inputs, and by utilizing a training method such as a [linear regression](https://en.wikipedia.org/wiki/Linear%5Fregression "Linear regression") or a [Ridge regression](https://en.wikipedia.org/wiki/Ridge%5Fregression "Ridge regression"). As its implementation depends on spatiotemporal reservoir patterns, the details of readout methods are tailored to each type of reservoir. For example, the readout for a reservoir computer using a container of liquid as its reservoir might entail observing spatiotemporal patterns on the surface of the liquid.

### Types

#### Context reverberation network

An early example of reservoir computing was the context reverberation network. In this architecture, an input layer feeds into a high dimensional dynamical system which is read out by a trainable single-layer [perceptron](https://en.wikipedia.org/wiki/Perceptron "Perceptron"). Two kinds of dynamical system were described: a recurrent neural network with fixed random weights, and a continuous [reaction–diffusion system](https://en.wikipedia.org/wiki/Reaction%E2%80%93diffusion%5Fsystem "Reaction–diffusion system") inspired by [Alan Turing](https://en.wikipedia.org/wiki/Alan%5FTuring "Alan Turing")’s model of [morphogenesis](https://en.wikipedia.org/wiki/Morphogenesis "Morphogenesis"). At the trainable layer, the perceptron associates current inputs with the signals that [reverberate](https://en.wikipedia.org/wiki/Reverberation "Reverberation") in the dynamical system; the latter were said to provide a dynamic "context" for the inputs. In the language of later work, the reaction–diffusion system served as the reservoir.

#### Echo state network

The Tree Echo State Network (TreeESN) model represents a generalization of the reservoir computing framework to tree structured data.

#### Liquid-state machine

**Chaotic Liquid State Machine** 

The liquid (i.e. reservoir) of a Chaotic Liquid State Machine (CLSM), or chaotic reservoir, is made from chaotic spiking neurons but which stabilize their activity by settling to a single hypothesis that describes the trained inputs of the machine. This is in contrast to general types of reservoirs that don’t stabilize. The liquid stabilization occurs via synaptic plasticity and chaos control that govern neural connections inside the liquid. CLSM showed promising results in learning sensitive time series data.

#### Nonlinear transient computation

This type of information processing is most relevant when time-dependent input signals depart from the mechanism’s internal dynamics. These departures cause transients or temporary altercations which are represented in the device’s output.

#### Deep reservoir computing

The extension of the reservoir computing framework towards Deep Learning, with the introduction of Deep Reservoir Computing and of the Deep Echo State Network (DeepESN) model allows to develop efficiently trained models for hierarchical processing of temporal data, at the same time enabling the investigation on the inherent role of layered composition in [recurrent neural networks](https://en.wikipedia.org/wiki/Recurrent%5Fneural%5Fnetwork "Recurrent neural network").

## Quantum reservoir computing

Quantum reservoir computing may use the nonlinear nature of quantum mechanical interactions or processes to form the characteristic nonlinear reservoirs but may also be done with linear reservoirs when the injection of the input to the reservoir creates the nonlinearity. The marriage of machine learning and quantum devices is leading to the emergence of quantum neuromorphic computing as a new research area.

### Types

#### Gaussian states of interacting quantum harmonic oscillators

Gaussian states are a paradigmatic class of states of [continuous variable quantum systems](https://en.wikipedia.org/wiki/Continuous-variable%5Fquantum%5Finformation "Continuous-variable quantum information"). Although they can nowadays be created and manipulated in, e.g, state-of-the-art optical platforms, naturally robust to [decoherence](https://en.wikipedia.org/wiki/Quantum%5Fdecoherence "Quantum decoherence"), it is well-known that they are not sufficient for, e.g., universal [quantum computing](https://en.wikipedia.org/wiki/Quantum%5Fcomputing "Quantum computing") because transformations that preserve the Gaussian nature of a state are linear. Normally, linear dynamics would not be sufficient for nontrivial reservoir computing either. It is nevertheless possible to harness such dynamics for reservoir computing purposes by considering a network of interacting [quantum harmonic oscillators](https://en.wikipedia.org/wiki/Quantum%5Fharmonic%5Foscillator "Quantum harmonic oscillator") and injecting the input by periodical state resets of a subset of the oscillators. With a suitable choice of how the states of this subset of oscillators depends on the input, the observables of the rest of the oscillators can become nonlinear functions of the input suitable for reservoir computing; indeed, thanks to the properties of these functions, even universal reservoir computing becomes possible by combining the observables with a polynomial readout function. In principle, such reservoir computers could be implemented with controlled multimode [optical parametric processes](https://en.wikipedia.org/wiki/Optical%5Fparametric%5Foscillator "Optical parametric oscillator"), however efficient extraction of the output from the system is challenging especially in the quantum regime where [measurement back-action](https://en.wikipedia.org/wiki/Measurement%5Fin%5Fquantum%5Fmechanics#State%5Fchange%5Fdue%5Fto%5Fmeasurement "Measurement in quantum mechanics") must be taken into account.

#### 2-D quantum dot lattices

In this architecture, randomized coupling between lattice sites grants the reservoir the “black box” property inherent to reservoir processors. The reservoir is then excited, which acts as the input, by an incident [optical field](https://en.wikipedia.org/wiki/Optical%5Ffield "Optical field"). Readout occurs in the form of occupational numbers of lattice sites, which are naturally nonlinear functions of the input.

#### Nuclear spins in a molecular solid

In this architecture, quantum mechanical coupling between spins of neighboring atoms within the [molecular solid](https://en.wikipedia.org/wiki/Molecular%5Fsolid "Molecular solid") provides the non-linearity required to create the higher-dimensional computational space. The reservoir is then excited by radiofrequency [electromagnetic radiation](https://en.wikipedia.org/wiki/Electromagnetic%5Fradiation "Electromagnetic radiation") tuned to the [resonance](https://en.wikipedia.org/wiki/Resonance "Resonance") frequencies of relevant [nuclear spins](https://en.wikipedia.org/wiki/Spin%5F%28physics%29 "Spin (physics)"). Readout occurs by measuring the nuclear spin states.

#### Reservoir computing on gate-based near-term superconducting quantum computers

The most prevalent model of quantum computing is the gate-based model where quantum computation is performed by sequential applications of unitary quantum gates on qubits of a quantum computer. A theory for the implementation of reservoir computing on a gate-based quantum computer with proof-of-principle demonstrations on a number of IBM superconducting [noisy intermediate-scale quantum](https://en.wikipedia.org/wiki/NISQ%5Fera "NISQ era") (NISQ) computers has been reported in.

## See also

* [Deep learning](https://en.wikipedia.org/wiki/Deep%5Flearning "Deep learning")
* [Extreme learning machines](https://en.wikipedia.org/wiki/Extreme%5Flearning%5Fmachine "Extreme learning machine")
* [Unconventional computing](https://en.wikipedia.org/wiki/Unconventional%5Fcomputing "Unconventional computing")

## References

1. ^ [Jump up to: _**a**_](#cite%5Fref-:4%5F1-0) [_**b**_](#cite%5Fref-:4%5F1-1) [_**c**_](#cite%5Fref-:4%5F1-2) [_**d**_](#cite%5Fref-:4%5F1-3) [_**e**_](#cite%5Fref-:4%5F1-4) [_**f**_](#cite%5Fref-:4%5F1-5) [_**g**_](#cite%5Fref-:4%5F1-6) [_**h**_](#cite%5Fref-:4%5F1-7) Tanaka, Gouhei; Yamane, Toshiyuki; Héroux, Jean Benoit; Nakane, Ryosho; Kanazawa, Naoki; Takeda, Seiji; Numata, Hidetoshi; Nakano, Daiju; Hirose, Akira (2019). ["Recent advances in physical reservoir computing: A review"](https://doi.org/10.1016%2Fj.neunet.2019.03.005). _Neural Networks_. **115**: 100–123\. [doi](https://en.wikipedia.org/wiki/Doi%5F%28identifier%29 "Doi (identifier)"):[10.1016/j.neunet.2019.03.005](https://doi.org/10.1016%2Fj.neunet.2019.03.005). [ISSN](https://en.wikipedia.org/wiki/ISSN%5F%28identifier%29 "ISSN (identifier)") [0893-6080](https://www.worldcat.org/issn/0893-6080). [PMID](https://en.wikipedia.org/wiki/PMID%5F%28identifier%29 "PMID (identifier)") [30981085](https://pubmed.ncbi.nlm.nih.gov/30981085).
2. **[^](#cite%5Fref-2 "Jump up")** Röhm, André; Lüdge, Kathy (2018-08-03). ["Multiplexed networks: reservoir computing with virtual and real nodes"](https://doi.org/10.1088%2F2399-6528%2Faad56d). _Journal of Physics Communications_. **2** (8): 085007\. [Bibcode](https://en.wikipedia.org/wiki/Bibcode%5F%28identifier%29 "Bibcode (identifier)"):[2018JPhCo...2h5007R](https://ui.adsabs.harvard.edu/abs/2018JPhCo...2h5007R). [doi](https://en.wikipedia.org/wiki/Doi%5F%28identifier%29 "Doi (identifier)"):[10.1088/2399-6528/aad56d](https://doi.org/10.1088%2F2399-6528%2Faad56d). [ISSN](https://en.wikipedia.org/wiki/ISSN%5F%28identifier%29 "ISSN (identifier)") [2399-6528](https://www.worldcat.org/issn/2399-6528).
3. ^ [Jump up to: _**a**_](#cite%5Fref-:0%5F3-0) [_**b**_](#cite%5Fref-:0%5F3-1) [_**c**_](#cite%5Fref-:0%5F3-2) [_**d**_](#cite%5Fref-:0%5F3-3) [_**e**_](#cite%5Fref-:0%5F3-4) Benjamin Schrauwen, David Verstraeten, and Jan Van Campenhout. "An overview of reservoir computing: theory, applications, and implementations." Proceedings of the European Symposium on Artificial Neural Networks ESANN 2007, pp. 471–482.
4. **[^](#cite%5Fref-4 "Jump up")** Fernando, C.; Sojakka, Sampsa (2003). "Pattern Recognition in a Bucket". _Advances in Artificial Life_. Lecture Notes in Computer Science. Vol. 2801\. pp. 588–597\. [doi](https://en.wikipedia.org/wiki/Doi%5F%28identifier%29 "Doi (identifier)"):[10.1007/978-3-540-39432-7\_63](https://doi.org/10.1007%2F978-3-540-39432-7%5F63). [ISBN](https://en.wikipedia.org/wiki/ISBN%5F%28identifier%29 "ISBN (identifier)") [978-3-540-20057-4](https://en.wikipedia.org/wiki/Special:BookSources/978-3-540-20057-4 "Special:BookSources/978-3-540-20057-4"). [S2CID](https://en.wikipedia.org/wiki/S2CID%5F%28identifier%29 "S2CID (identifier)") [15073928](https://api.semanticscholar.org/CorpusID:15073928).
5. ^ [Jump up to: _**a**_](#cite%5Fref-:2%5F5-0) [_**b**_](#cite%5Fref-:2%5F5-1) [_**c**_](#cite%5Fref-:2%5F5-2) [_**d**_](#cite%5Fref-:2%5F5-3) [_**e**_](#cite%5Fref-:2%5F5-4) Ghosh, Sanjib; Opala, Andrzej; Matuszewski, Michał; Paterek, Tomasz; Liew, Timothy C. H. (December 2019). "Quantum reservoir processing". _npj Quantum Information_. **5** (1): 35\. [arXiv](https://en.wikipedia.org/wiki/ArXiv%5F%28identifier%29 "ArXiv (identifier)"):[1811.10335](https://arxiv.org/abs/1811.10335). [Bibcode](https://en.wikipedia.org/wiki/Bibcode%5F%28identifier%29 "Bibcode (identifier)"):[2019npjQI...5...35G](https://ui.adsabs.harvard.edu/abs/2019npjQI...5...35G). [doi](https://en.wikipedia.org/wiki/Doi%5F%28identifier%29 "Doi (identifier)"):[10.1038/s41534-019-0149-8](https://doi.org/10.1038%2Fs41534-019-0149-8). [ISSN](https://en.wikipedia.org/wiki/ISSN%5F%28identifier%29 "ISSN (identifier)") [2056-6387](https://www.worldcat.org/issn/2056-6387). [S2CID](https://en.wikipedia.org/wiki/S2CID%5F%28identifier%29 "S2CID (identifier)") [119197635](https://api.semanticscholar.org/CorpusID:119197635).
6. ^ [Jump up to: _**a**_](#cite%5Fref-:3%5F6-0) [_**b**_](#cite%5Fref-:3%5F6-1) [_**c**_](#cite%5Fref-:3%5F6-2) [_**d**_](#cite%5Fref-:3%5F6-3) [_**e**_](#cite%5Fref-:3%5F6-4) [_**f**_](#cite%5Fref-:3%5F6-5) [_**g**_](#cite%5Fref-:3%5F6-6) [_**h**_](#cite%5Fref-:3%5F6-7) Negoro, Makoto; Mitarai, Kosuke; Fujii, Keisuke; Nakajima, Kohei; Kitagawa, Masahiro (2018-06-28). "Machine learning with controllable quantum dynamics of a nuclear spin ensemble in a solid". [arXiv](https://en.wikipedia.org/wiki/ArXiv%5F%28identifier%29 "ArXiv (identifier)"):[1806.10910](https://arxiv.org/abs/1806.10910) \[[quant-ph](https://arxiv.org/archive/quant-ph)\].
7. **[^](#cite%5Fref-RB08%5F7-0 "Jump up")** Rahimi, Ali; Recht, Benjamin (December 2008). ["Weighted Sums of Random Kitchen Sinks: Replacing minimization with randomization in Learning"](http://papers.nips.cc/paper/3495-weighted-sums-of-random-kitchen-sinks-replacing-minimization-with-randomization-in-learning.pdf) (PDF). _NIPS'08: Proceedings of the 21st International Conference on Neural Information Processing Systems_: 1313–1320.
8. ^ [Jump up to: _**a**_](#cite%5Fref-JNY20%5F8-0) [_**b**_](#cite%5Fref-JNY20%5F8-1) [_**c**_](#cite%5Fref-JNY20%5F8-2) Chen, Jiayin; Nurdin, Hendra; Yamamoto, Naoki (2020-08-24). ["Temporal Information Processing on Noisy Quantum Computers"](https://doi.org/10.1103/PhysRevApplied.14.024065). _Physical Review Applied_. **14** (2): 024065\. [arXiv](https://en.wikipedia.org/wiki/ArXiv%5F%28identifier%29 "ArXiv (identifier)"):[2001.09498](https://arxiv.org/abs/2001.09498). [Bibcode](https://en.wikipedia.org/wiki/Bibcode%5F%28identifier%29 "Bibcode (identifier)"):[2020PhRvP..14b4065C](https://ui.adsabs.harvard.edu/abs/2020PhRvP..14b4065C). [doi](https://en.wikipedia.org/wiki/Doi%5F%28identifier%29 "Doi (identifier)"):[10.1103/PhysRevApplied.14.024065](https://doi.org/10.1103%2FPhysRevApplied.14.024065). [S2CID](https://en.wikipedia.org/wiki/S2CID%5F%28identifier%29 "S2CID (identifier)") [210920543](https://api.semanticscholar.org/CorpusID:210920543).
9. **[^](#cite%5Fref-9 "Jump up")** Pathak, Jaideep; Hunt, Brian; Girvan, Michelle; Lu, Zhixin; Ott, Edward (2018-01-12). ["Model-Free Prediction of Large Spatiotemporally Chaotic Systems from Data: A Reservoir Computing Approach"](https://doi.org/10.1103%2FPhysRevLett.120.024102). _Physical Review Letters_. **120** (2): 024102\. [Bibcode](https://en.wikipedia.org/wiki/Bibcode%5F%28identifier%29 "Bibcode (identifier)"):[2018PhRvL.120b4102P](https://ui.adsabs.harvard.edu/abs/2018PhRvL.120b4102P). [doi](https://en.wikipedia.org/wiki/Doi%5F%28identifier%29 "Doi (identifier)"):[10.1103/PhysRevLett.120.024102](https://doi.org/10.1103%2FPhysRevLett.120.024102). [PMID](https://en.wikipedia.org/wiki/PMID%5F%28identifier%29 "PMID (identifier)") [29376715](https://pubmed.ncbi.nlm.nih.gov/29376715).
10. **[^](#cite%5Fref-10 "Jump up")** Vlachas, P.R.; Pathak, J.; Hunt, B.R.; Sapsis, T.P.; Girvan, M.; Ott, E.; Koumoutsakos, P. (2020-03-21). ["Backpropagation algorithms and Reservoir Computing in Recurrent Neural Networks for the forecasting of complex spatiotemporal dynamics"](https://dx.doi.org/10.1016/j.neunet.2020.02.016). _Neural Networks_. **126**: 191–217\. [arXiv](https://en.wikipedia.org/wiki/ArXiv%5F%28identifier%29 "ArXiv (identifier)"):[1910.05266](https://arxiv.org/abs/1910.05266). [doi](https://en.wikipedia.org/wiki/Doi%5F%28identifier%29 "Doi (identifier)"):[10.1016/j.neunet.2020.02.016](https://doi.org/10.1016%2Fj.neunet.2020.02.016). [ISSN](https://en.wikipedia.org/wiki/ISSN%5F%28identifier%29 "ISSN (identifier)") [0893-6080](https://www.worldcat.org/issn/0893-6080). [PMID](https://en.wikipedia.org/wiki/PMID%5F%28identifier%29 "PMID (identifier)") [32248008](https://pubmed.ncbi.nlm.nih.gov/32248008). [S2CID](https://en.wikipedia.org/wiki/S2CID%5F%28identifier%29 "S2CID (identifier)") [211146609](https://api.semanticscholar.org/CorpusID:211146609).
11. **[^](#cite%5Fref-11 "Jump up")** Krishnagopal, Sanjukta; Girvan, Michelle; Ott, Edward; Hunt, Brian R. (2020-02-01). ["Separation of chaotic signals by reservoir computing"](https://aip.scitation.org/doi/10.1063/1.5132766). _Chaos: An Interdisciplinary Journal of Nonlinear Science_. **30** (2): 023123\. [arXiv](https://en.wikipedia.org/wiki/ArXiv%5F%28identifier%29 "ArXiv (identifier)"):[1910.10080](https://arxiv.org/abs/1910.10080). [Bibcode](https://en.wikipedia.org/wiki/Bibcode%5F%28identifier%29 "Bibcode (identifier)"):[2020Chaos..30b3123K](https://ui.adsabs.harvard.edu/abs/2020Chaos..30b3123K). [doi](https://en.wikipedia.org/wiki/Doi%5F%28identifier%29 "Doi (identifier)"):[10.1063/1.5132766](https://doi.org/10.1063%2F1.5132766). [ISSN](https://en.wikipedia.org/wiki/ISSN%5F%28identifier%29 "ISSN (identifier)") [1054-1500](https://www.worldcat.org/issn/1054-1500). [PMID](https://en.wikipedia.org/wiki/PMID%5F%28identifier%29 "PMID (identifier)") [32113243](https://pubmed.ncbi.nlm.nih.gov/32113243). [S2CID](https://en.wikipedia.org/wiki/S2CID%5F%28identifier%29 "S2CID (identifier)") [204823815](https://api.semanticscholar.org/CorpusID:204823815).
12. **[^](#cite%5Fref-12 "Jump up")** Banerjee, Amitava; Hart, Joseph D.; Roy, Rajarshi; Ott, Edward (2021-07-20). ["Machine Learning Link Inference of Noisy Delay-Coupled Networks with Optoelectronic Experimental Tests"](https://doi.org/10.1103%2FPhysRevX.11.031014). _Physical Review X_. **11** (3): 031014\. [arXiv](https://en.wikipedia.org/wiki/ArXiv%5F%28identifier%29 "ArXiv (identifier)"):[2010.15289](https://arxiv.org/abs/2010.15289). [Bibcode](https://en.wikipedia.org/wiki/Bibcode%5F%28identifier%29 "Bibcode (identifier)"):[2021PhRvX..11c1014B](https://ui.adsabs.harvard.edu/abs/2021PhRvX..11c1014B). [doi](https://en.wikipedia.org/wiki/Doi%5F%28identifier%29 "Doi (identifier)"):[10.1103/PhysRevX.11.031014](https://doi.org/10.1103%2FPhysRevX.11.031014).
13. ^ [Jump up to: _**a**_](#cite%5Fref-:1%5F13-0) [_**b**_](#cite%5Fref-:1%5F13-1) [_**c**_](#cite%5Fref-:1%5F13-2) [_**d**_](#cite%5Fref-:1%5F13-3) Soriano, Miguel C. (2017-02-06). ["Viewpoint: Reservoir Computing Speeds Up"](https://physics.aps.org/articles/v10/12). _Physics_. **10**: 12\. [doi](https://en.wikipedia.org/wiki/Doi%5F%28identifier%29 "Doi (identifier)"):[10.1103/Physics.10.12](https://doi.org/10.1103%2FPhysics.10.12).
14. **[^](#cite%5Fref-14 "Jump up")** Kevin Kirby. "Context dynamics in neural sequential learning." Proceedings of the Florida Artificial Intelligence Research Symposium FLAIRS (1991), 66–70.
15. **[^](#cite%5Fref-15 "Jump up")** Gallicchio, Claudio; Micheli, Alessio (2013). "Tree Echo State Networks". _Neurocomputing_. **101**: 319–337\. [doi](https://en.wikipedia.org/wiki/Doi%5F%28identifier%29 "Doi (identifier)"):[10.1016/j.neucom.2012.08.017](https://doi.org/10.1016%2Fj.neucom.2012.08.017). [hdl](https://en.wikipedia.org/wiki/Hdl%5F%28identifier%29 "Hdl (identifier)"):[11568/158480](https://hdl.handle.net/11568%2F158480).
16. ^ [Jump up to: _**a**_](#cite%5Fref-:7%5F16-0) [_**b**_](#cite%5Fref-:7%5F16-1) Aoun, Mario Antoine; Boukadoum, Mounir (2014). ["Learning algorithm and neurocomputing architecture for NDS Neurons"](https://dx.doi.org/10.1109/icci-cc.2014.6921451). _2014 IEEE 13th International Conference on Cognitive Informatics and Cognitive Computing_. IEEE. pp. 126–132\. [doi](https://en.wikipedia.org/wiki/Doi%5F%28identifier%29 "Doi (identifier)"):[10.1109/icci-cc.2014.6921451](https://doi.org/10.1109%2Ficci-cc.2014.6921451). [ISBN](https://en.wikipedia.org/wiki/ISBN%5F%28identifier%29 "ISBN (identifier)") [978-1-4799-6081-1](https://en.wikipedia.org/wiki/Special:BookSources/978-1-4799-6081-1 "Special:BookSources/978-1-4799-6081-1"). [S2CID](https://en.wikipedia.org/wiki/S2CID%5F%28identifier%29 "S2CID (identifier)") [16026952](https://api.semanticscholar.org/CorpusID:16026952).
17. ^ [Jump up to: _**a**_](#cite%5Fref-:8%5F17-0) [_**b**_](#cite%5Fref-:8%5F17-1) Aoun, Mario Antoine; Boukadoum, Mounir (2015). ["Chaotic Liquid State Machine"](https://dx.doi.org/10.4018/ijcini.2015100101). _International Journal of Cognitive Informatics and Natural Intelligence_. **9** (4): 1–20\. [doi](https://en.wikipedia.org/wiki/Doi%5F%28identifier%29 "Doi (identifier)"):[10.4018/ijcini.2015100101](https://doi.org/10.4018%2Fijcini.2015100101). [ISSN](https://en.wikipedia.org/wiki/ISSN%5F%28identifier%29 "ISSN (identifier)") [1557-3958](https://www.worldcat.org/issn/1557-3958).
18. ^ [Jump up to: _**a**_](#cite%5Fref-NTC%5F18-0) [_**b**_](#cite%5Fref-NTC%5F18-1) Crook, Nigel (2007). "Nonlinear Transient Computation". _Neurocomputing_. **70** (7–9): 1167–1176\. [doi](https://en.wikipedia.org/wiki/Doi%5F%28identifier%29 "Doi (identifier)"):[10.1016/j.neucom.2006.10.148](https://doi.org/10.1016%2Fj.neucom.2006.10.148).
19. **[^](#cite%5Fref-19 "Jump up")** Pedrelli, Luca (2019). [_Deep Reservoir Computing: A Novel Class of Deep Recurrent Neural Networks_](https://etd.adm.unipi.it/t/etd-02282019-191815/) (PhD thesis). Università di Pisa.
20. **[^](#cite%5Fref-20 "Jump up")** Gallicchio, Claudio; Micheli, Alessio; Pedrelli, Luca (2017-12-13). "Deep reservoir computing: A critical experimental analysis". _Neurocomputing_. **268**: 87–99\. [doi](https://en.wikipedia.org/wiki/Doi%5F%28identifier%29 "Doi (identifier)"):[10.1016/j.neucom.2016.12.089](https://doi.org/10.1016%2Fj.neucom.2016.12.089). [hdl](https://en.wikipedia.org/wiki/Hdl%5F%28identifier%29 "Hdl (identifier)"):[11568/851934](https://hdl.handle.net/11568%2F851934).
21. **[^](#cite%5Fref-21 "Jump up")** Gallicchio, Claudio; Micheli, Alessio (2017-05-05). "Echo State Property of Deep Reservoir Computing Networks". _Cognitive Computation_. **9** (3): 337–350\. [doi](https://en.wikipedia.org/wiki/Doi%5F%28identifier%29 "Doi (identifier)"):[10.1007/s12559-017-9461-9](https://doi.org/10.1007%2Fs12559-017-9461-9). [hdl](https://en.wikipedia.org/wiki/Hdl%5F%28identifier%29 "Hdl (identifier)"):[11568/851932](https://hdl.handle.net/11568%2F851932). [ISSN](https://en.wikipedia.org/wiki/ISSN%5F%28identifier%29 "ISSN (identifier)") [1866-9956](https://www.worldcat.org/issn/1866-9956). [S2CID](https://en.wikipedia.org/wiki/S2CID%5F%28identifier%29 "S2CID (identifier)") [1077549](https://api.semanticscholar.org/CorpusID:1077549).
22. **[^](#cite%5Fref-22 "Jump up")** Gallicchio, Claudio; Micheli, Alessio; Pedrelli, Luca (December 2018). "Design of deep echo state networks". _Neural Networks_. **108**: 33–47\. [doi](https://en.wikipedia.org/wiki/Doi%5F%28identifier%29 "Doi (identifier)"):[10.1016/j.neunet.2018.08.002](https://doi.org/10.1016%2Fj.neunet.2018.08.002). [hdl](https://en.wikipedia.org/wiki/Hdl%5F%28identifier%29 "Hdl (identifier)"):[11568/939082](https://hdl.handle.net/11568%2F939082). [ISSN](https://en.wikipedia.org/wiki/ISSN%5F%28identifier%29 "ISSN (identifier)") [0893-6080](https://www.worldcat.org/issn/0893-6080). [PMID](https://en.wikipedia.org/wiki/PMID%5F%28identifier%29 "PMID (identifier)") [30138751](https://pubmed.ncbi.nlm.nih.gov/30138751). [S2CID](https://en.wikipedia.org/wiki/S2CID%5F%28identifier%29 "S2CID (identifier)") [52075702](https://api.semanticscholar.org/CorpusID:52075702).
23. **[^](#cite%5Fref-CN19%5F23-0 "Jump up")** Chen, Jiayin; Nurdin, Hendra (2019-05-15). ["Learning nonlinear input–output maps with dissipative quantum systems"](https://link.springer.com/article/10.1007%2Fs11128-019-2311-9). _Quantum Information Processing_. **18** (7): 198\. [arXiv](https://en.wikipedia.org/wiki/ArXiv%5F%28identifier%29 "ArXiv (identifier)"):[1901.01653](https://arxiv.org/abs/1901.01653). [Bibcode](https://en.wikipedia.org/wiki/Bibcode%5F%28identifier%29 "Bibcode (identifier)"):[2019QuIP...18..198C](https://ui.adsabs.harvard.edu/abs/2019QuIP...18..198C). [doi](https://en.wikipedia.org/wiki/Doi%5F%28identifier%29 "Doi (identifier)"):[10.1007/s11128-019-2311-9](https://doi.org/10.1007%2Fs11128-019-2311-9). [S2CID](https://en.wikipedia.org/wiki/S2CID%5F%28identifier%29 "S2CID (identifier)") [57573677](https://api.semanticscholar.org/CorpusID:57573677).
24. ^ [Jump up to: _**a**_](#cite%5Fref-:5%5F24-0) [_**b**_](#cite%5Fref-:5%5F24-1) Nokkala, Johannes; Martínez-Peña, Rodrigo; Giorgi, Gian Luca; Parigi, Valentina; Soriano, Miguel C.; Zambrini, Roberta (2021). "Gaussian states of continuous-variable quantum systems provide universal and versatile reservoir computing". _Communications Physics_. **4** (1): 53\. [arXiv](https://en.wikipedia.org/wiki/ArXiv%5F%28identifier%29 "ArXiv (identifier)"):[2006.04821](https://arxiv.org/abs/2006.04821). [Bibcode](https://en.wikipedia.org/wiki/Bibcode%5F%28identifier%29 "Bibcode (identifier)"):[2021CmPhy...4...53N](https://ui.adsabs.harvard.edu/abs/2021CmPhy...4...53N). [doi](https://en.wikipedia.org/wiki/Doi%5F%28identifier%29 "Doi (identifier)"):[10.1038/s42005-021-00556-w](https://doi.org/10.1038%2Fs42005-021-00556-w). [S2CID](https://en.wikipedia.org/wiki/S2CID%5F%28identifier%29 "S2CID (identifier)") [234355683](https://api.semanticscholar.org/CorpusID:234355683).
25. **[^](#cite%5Fref-MG20%5F25-0 "Jump up")** Marković, Danijela; Grollier, Julie (2020-10-13). ["Quantum Neuromorphic Computing"](https://doi.org/10.1063/5.0020014). _Applied Physics Letters_. **117** (15): 150501\. [arXiv](https://en.wikipedia.org/wiki/ArXiv%5F%28identifier%29 "ArXiv (identifier)"):[2006.15111](https://arxiv.org/abs/2006.15111). [Bibcode](https://en.wikipedia.org/wiki/Bibcode%5F%28identifier%29 "Bibcode (identifier)"):[2020ApPhL.117o0501M](https://ui.adsabs.harvard.edu/abs/2020ApPhL.117o0501M). [doi](https://en.wikipedia.org/wiki/Doi%5F%28identifier%29 "Doi (identifier)"):[10.1063/5.0020014](https://doi.org/10.1063%2F5.0020014). [S2CID](https://en.wikipedia.org/wiki/S2CID%5F%28identifier%29 "S2CID (identifier)") [210920543](https://api.semanticscholar.org/CorpusID:210920543).
26. **[^](#cite%5Fref-26 "Jump up")** Ferraro, Alessandro; Olivares, Stefano; Paris, Matteo G. A. (2005-03-31). "Gaussian states in continuous variable quantum information". [arXiv](https://en.wikipedia.org/wiki/ArXiv%5F%28identifier%29 "ArXiv (identifier)"):[quant-ph/0503237](https://arxiv.org/abs/quant-ph/0503237).
27. **[^](#cite%5Fref-27 "Jump up")** Roslund, Jonathan; de Araújo, Renné Medeiros; Jiang, Shifeng; Fabre, Claude; Treps, Nicolas (2013-12-15). ["Wavelength-multiplexed quantum networks with ultrafast frequency combs"](https://www.nature.com/articles/nphoton.2013.340). _Nature Photonics_. **8** (2): 109–112\. [arXiv](https://en.wikipedia.org/wiki/ArXiv%5F%28identifier%29 "ArXiv (identifier)"):[1307.1216](https://arxiv.org/abs/1307.1216). [doi](https://en.wikipedia.org/wiki/Doi%5F%28identifier%29 "Doi (identifier)"):[10.1038/nphoton.2013.340](https://doi.org/10.1038%2Fnphoton.2013.340). [ISSN](https://en.wikipedia.org/wiki/ISSN%5F%28identifier%29 "ISSN (identifier)") [1749-4893](https://www.worldcat.org/issn/1749-4893). [S2CID](https://en.wikipedia.org/wiki/S2CID%5F%28identifier%29 "S2CID (identifier)") [2328402](https://api.semanticscholar.org/CorpusID:2328402).
28. **[^](#cite%5Fref-28 "Jump up")** Bartlett, Stephen D.; Sanders, Barry C.; Braunstein, Samuel L.; Nemoto, Kae (2002-02-14). ["Efficient Classical Simulation of Continuous Variable Quantum Information Processes"](https://link.aps.org/doi/10.1103/PhysRevLett.88.097904). _Physical Review Letters_. **88** (9): 097904\. [arXiv](https://en.wikipedia.org/wiki/ArXiv%5F%28identifier%29 "ArXiv (identifier)"):[quant-ph/0109047](https://arxiv.org/abs/quant-ph/0109047). [Bibcode](https://en.wikipedia.org/wiki/Bibcode%5F%28identifier%29 "Bibcode (identifier)"):[2002PhRvL..88i7904B](https://ui.adsabs.harvard.edu/abs/2002PhRvL..88i7904B). [doi](https://en.wikipedia.org/wiki/Doi%5F%28identifier%29 "Doi (identifier)"):[10.1103/PhysRevLett.88.097904](https://doi.org/10.1103%2FPhysRevLett.88.097904). [PMID](https://en.wikipedia.org/wiki/PMID%5F%28identifier%29 "PMID (identifier)") [11864057](https://pubmed.ncbi.nlm.nih.gov/11864057). [S2CID](https://en.wikipedia.org/wiki/S2CID%5F%28identifier%29 "S2CID (identifier)") [2161585](https://api.semanticscholar.org/CorpusID:2161585).
29. **[^](#cite%5Fref-29 "Jump up")** Nokkala, J.; Arzani, F.; Galve, F.; Zambrini, R.; Maniscalco, S.; Piilo, J.; Treps, N.; Parigi, V. (2018-05-09). ["Reconfigurable optical implementation of quantum complex networks"](https://doi.org/10.1088%2F1367-2630%2Faabc77). _New Journal of Physics_. **20** (5): 053024\. [arXiv](https://en.wikipedia.org/wiki/ArXiv%5F%28identifier%29 "ArXiv (identifier)"):[1708.08726](https://arxiv.org/abs/1708.08726). [Bibcode](https://en.wikipedia.org/wiki/Bibcode%5F%28identifier%29 "Bibcode (identifier)"):[2018NJPh...20e3024N](https://ui.adsabs.harvard.edu/abs/2018NJPh...20e3024N). [doi](https://en.wikipedia.org/wiki/Doi%5F%28identifier%29 "Doi (identifier)"):[10.1088/1367-2630/aabc77](https://doi.org/10.1088%2F1367-2630%2Faabc77). [ISSN](https://en.wikipedia.org/wiki/ISSN%5F%28identifier%29 "ISSN (identifier)") [1367-2630](https://www.worldcat.org/issn/1367-2630). [S2CID](https://en.wikipedia.org/wiki/S2CID%5F%28identifier%29 "S2CID (identifier)") [119091176](https://api.semanticscholar.org/CorpusID:119091176).
30. **[^](#cite%5Fref-30 "Jump up")** Nielsen, Michael; Chuang, Isaac (2010), _Quantum Computation and Quantum Information_ (2 ed.), Cambridge University Press Cambridge
31. **[^](#cite%5Fref-31 "Jump up")** [John Preskill](https://en.wikipedia.org/wiki/John%5FPreskill "John Preskill"). "Quantum Computing in the NISQ era and beyond." Quantum 2,79 (2018)

## Further reading

* [Reservoir Computing using delay systems](http://www.nature.com/ncomms/journal/v2/n9/full/ncomms1476.html?WT.ec%5Fid=NCOMMS-20110913), Nature Communications 2011
* [Optoelectronic Reservoir Computing](http://www.nature.com/srep/2012/120227/srep00287/full/srep00287.html), Scientific Reports February 2012
* [Optoelectronic Reservoir Computing](http://www.opticsinfobase.org/oe/abstract.cfm?uri=oe-20-3-3241), Optics Express 2012
* [All-optical Reservoir Computing](http://www.nature.com/ncomms/journal/v4/n1/full/ncomms2368.html), Nature Communications 2013
* [Memristor Models for Machine learning](http://www.mitpressjournals.org/doi/10.1162/NECO%5Fa%5F00694#.WL4P9iHyvIo), Neural Computation 2014 [arxiv](https://arxiv.org/abs/1406.2210)

---

