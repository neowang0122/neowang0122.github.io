---
id: 35fb2f80-a9ee-11ee-8183-c7d567937531
title: |
  提示词工程的问题视角 - 少数派
author: |
  叶猛犸
tags:
  - RSS
date_saved: 2024-01-02 22:00:00
date_published: 2024-01-02 22:00:00
draft: true
---

# 提示词工程的问题视角 - 少数派
#Omnivore

[Read on Omnivore](https://omnivore.app/me/-18ccd852deb)

[Read Original](https://sspai.com/post/85484)

date_saved: 2024-01-02 22:00:00

date_published: 2024-01-02 22:00:00

--- 

# Full Content: 

**Matrix 首页推荐** 

[Matrix](https://sspai.com/matrix) 是少数派的写作社区，我们主张分享真实的产品体验，有实用价值的经验与思考。我们会不定期挑选 Matrix 最优质的文章，展示来自用户的最真实的体验和观点。

文章代表作者个人观点，少数派仅对标题和排版略作修改。

---

在刚刚过去的 12 月，OpenAI 发布了一份[提示词工程指南](https://sspai.com/link?target=https%3A%2F%2Fplatform.openai.com%2Fdocs%2Fguides%2Fprompt-engineering)。这份文档列出了一些和大语言模型助手沟通的基本原则，包括指令要清晰、复杂任务要拆分、最好给出示例、给出操作流程等等。

这些基本规则，和今年 4 月份吴恩达教授的短课程相似——那门课叫[《面向开发者的提示词工程》](https://sspai.com/link?target=https%3A%2F%2Fwww.deeplearning.ai%2Fshort-courses%2Fchatgpt-prompt-engineering-for-developers%2F)。不过，在吴恩达教授的课程中，介绍的是用大模型开发应用的基本原则，以及大模型的几类常见应用：总结、判断、转换、扩展。容易看出，其中并没有包括人们常见的使用大模型的方式：**问答**。

人们希望大语言模型能直接给出答案。这是更为直觉的用法，或者说，人们希望能像使用搜索引擎那样使用大语言模型，或者像靠谱的人类助手相似，能够三言两语就完成沟通，而不是先做一堆准备工作，先写一篇小作文。

不过这可能难以在短期内实现。提示词工程（Prompt Engineering），目前还是有必要存在的。

![prompt](https://proxy-prod.omnivore-image-cache.app/0x0,sYilx6n_q-GTyskP6eG-uPtn0u3aLlhVRwXeIAXnCGtk/https://cdn.sspai.com/2023/12/31/article/cd1bb4781e1edc8e38a06e08a9d817c1?imageView2/2/w/1120/q/90/interlace/1/ignore-error/1)

捎带说明一下，本文主要讨论的是大语言模型中的提示词工程，因此文中没有使用「生成式人工智能」这个范围更大的术语，而全部都使用了「大语言模型」。

## 什么是提示词工程

> 没想到答案，就不要寻找题目。

但凡日常生活中接触到、使用过一些大语言模型产品的人，应该对提示词工程都不陌生。提示词工程是指通过设计和优化输入提示，引导大语言模型更有效、更精确地生成所需的内容。

![prompt](https://proxy-prod.omnivore-image-cache.app/0x0,sSx4w4HI871ZuMEJL4yAJh0i9JtPbz5qspGudTMfCGcQ/https://cdn.sspai.com/2023/12/31/article/fa876a67f238382e2446037c33791d17?imageView2/2/w/1120/q/90/interlace/1/ignore-error/1)

[@宝玉xp](https://weibo.com/u/1727858283) 设计了一段效果很好的[翻译提示词](https://weibo.com/1727858283/NlsDSpPaa)：

> 你是一位精通简体中文的专业翻译，曾参与《纽约时报》和《经济学人》中文版的翻译工作，因此对于新闻和时事文章的翻译有深入的理解。我希望你能帮我将以下英文新闻段落翻译成中文，风格与上述杂志的中文版相似。
> 
> 规则：
> 
> * 翻译时要准确传达新闻事实和背景。
> * 保留特定的英文术语或名字，并在其前后加上空格，例如：「中 UN 文」。
> * 分成两次翻译，并且打印每一次结果：
> 1. 根据新闻内容直译，不要遗漏任何信息
> 2. 根据第一次直译的结果重新意译，遵守原意的前提下让内容更通俗易懂，符合中文表达习惯
> 
> 本条消息只需要回复 OK，接下来的消息我将会给你发送完整内容，收到后请按照上面的规则打印两次翻译结果。

这段提示词中包含了角色扮演、规则设定，以及让大语言模型自行回顾和改善。显然，这和人类日常对话的方式不同。这也就意味着，提示词工程是一种需要学习才能掌握的技能——虽然门槛不高，但是还是要学习的。

这和人们期盼的「使用 AI 应该像和人聊天」，显然差距不小。

有一种观点认为，随着 AI 大模型的能力越来越强，对真实世界的理解越来越多，人们将会越来越容易使用自然语言与 AI 沟通，提示词工程将不再有用武之地。2022 年 9 月 13 日，OpenAI 的 Sam Altman 和 LinkedIn 的 Reid Hoffman 有一场[对谈](https://sspai.com/link?target=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DWHoWGNQRXb0%26t%3D1523s)。Sam Altman 认为提示词工程是个短期需求：

> 「5 年后我们将不再需要提示词工程，或者只需在这方面做少量工作。将来的 AI 系统不会因为增补了某个特定词就会产生截然不同的输出，而是可以较好地理解自然语言，用户只需以文本和语音形式输入指令，即可让计算机完成图像生成、资料研究、心理咨询等复杂任务。」
> 
> 「总的来说，用户只须使用自然语言就可以与计算机交互，当然，如果艺术家能想出更有创造性的描述，也自然就可以生成更好的图像。」

AI 大佬[杨立昆](https://sspai.com/link?target=https%3A%2F%2Fen.wikipedia.org%2Fwiki%2FYann%5FLeCun)也支持这种观点。他认为，之所以需要提示词工程，是因为现在的大语言模型的理解能力还比较弱。

![Yann LeCun](https://proxy-prod.omnivore-image-cache.app/0x0,sLRMC004TeZV-ADZANtgKeVl7Vynof4BxDAW8XrCS5Sw/https://cdn.sspai.com/2023/12/31/article/2e5f2d129410b299eeecf176f6e4c625?imageView2/2/w/1120/q/90/interlace/1/ignore-error/1)

![Yann LeCun](https://proxy-prod.omnivore-image-cache.app/0x0,sq03qyZ92FOjyjSwBvq3lzQgNuIO8MHiVrLip9EIAYcY/https://cdn.sspai.com/2023/12/31/article/2a3600212cafb032be3f431354f09daa?imageView2/2/w/1120/q/90/interlace/1/ignore-error/1)

而另一种观点则认为，考虑 AI 大模型的基本技术原理，提示词工程将会一直存在，而且将成为重要的技术，相当于大语言模型出现之前的编程技术一样。Sam Altman 似乎改变了自己的观点；2023 年 2 月 21 日，他发了一条 Twitter：「写出好提示词是一项高杠杆技能。」

![Sam Altman](https://proxy-prod.omnivore-image-cache.app/0x0,s_L9SwaZPz9bE4Md5Z53T7nJwpsJHJNqVmARPSeUONFw/https://cdn.sspai.com/2023/12/31/article/316b0357ef969e2ea405c8181561291e?imageView2/2/w/1120/q/90/interlace/1/ignore-error/1)

这个观点和今年上半年的许多人相同。[不少企业](https://sspai.com/link?target=https%3A%2F%2Fwww.linkedin.com%2Fjobs%2Fprompt-engineer-jobs%2F%3FcurrentJobId%3D3690188198)开始招聘「提示词工程师」，提示词工程课程、网站和插件纷纷涌现。虽然生成式人工智能的能力在不断提升，但它们仍然不能理解人类语言的真正含义。大语言模型的原理是统计和概率，而非真正意义上的理解。因此，即使能更好地处理自然语言，但在特定场景下生成高质量、精确的内容仍需精心设计的提示。

换言之，大语言模型学习了许多东西，也能做一些推理和判断，但是它不知道用户面临的具体情境。我们要使用大语言模型来解决问题，需要自己先整理好问题，把它精确描述出来——将存在于思想中、甚至自己意识不到的需求，转化为明确的文本请求。

这样可以解决两个问题：上下文和歧义。前者是因为问题背景的复杂，后者是因为人类语言的模糊。

真正的问题总是复杂的，而人们在日常沟通中已经习惯了双方共同默认的问题背景和情境——这就是为什么网上陌生人沟通往往会吵起来。而大语言模型并没有这些个人化的背景知识。复杂总需要在某个地方得到满足，或者是用户界面（想想拥有无数菜单和可选项的软件），或者是解决方案。ChatGPT 提供了一个简单的用户界面，于是复杂就转移到了解决方案上。结果就是，我们需要跟大模型明确说明它需要解决的问题。

除了这两个问题之外，还有一个更不容易解决的问题：幻觉。

## 大模型是场梦

> 世事一场大梦，人生几度秋凉。

今年剑桥词典选出的年度词是 hallucinate。这是个动词，意思是「产生幻觉」，特指大语言模型一本正经地胡说八道。

![Cambridge Dictionary](https://proxy-prod.omnivore-image-cache.app/0x0,sujABzYJ_3tiLWk_AorzJ-3shn_QtX_8hRrn5Id9Yd_A/https://cdn.sspai.com/2023/12/31/article/6974ceee937d5c2bd02f2c8e822a0f7b?imageView2/2/w/1120/q/90/interlace/1/ignore-error/1)

12 月初，OpenAI 的计算机科学家 [Andrej Karpathy](https://sspai.com/link?target=https%3A%2F%2Fkarpathy.ai%2F) 写了篇[短文](https://sspai.com/link?target=https%3A%2F%2Ftwitter.com%2Fkarpathy%2Fstatus%2F1733299213503787018)，讨论了大语言模型的幻觉问题。他认为，本质上来看，大语言模型就是在做梦。幻觉不是大模型的问题，它是大模型的工作方式。或者说，它不是大语言模型的缺陷，而是大语言模型的特征。

![Andrej Karpathy](https://proxy-prod.omnivore-image-cache.app/0x0,shFfkldaOv7rsh9mKJ5Q_HxDYlMtnTxPgKxF4wbm4GHQ/https://cdn.sspai.com/2023/12/31/article/a08687f61543ae42b65ab1c7c3961bb9?imageView2/2/w/1120/q/90/interlace/1/ignore-error/1)

It’s not a Bug, it’s a Feature!

![Andrej Karpathy](https://proxy-prod.omnivore-image-cache.app/0x0,sLcAeYEKhCPM7S9pf99bevsrEVkoskFtJCY8VeyCiYO8/https://cdn.sspai.com/2023/12/31/article/83ac341e032e27e8f43fa215e32033bb?imageView2/2/w/1120/q/40/interlace/1/ignore-error/1)

如果把大语言模型视为光谱的一端，另一端则是搜索引擎。搜索引擎完全不做梦，它根据用户的输入内容精确查找，它没有幻觉，但也没有生成内容的能力。把大语言模型当成搜索引擎或者相反，都是错误的。

Karpathy 认为，当人们将大语言模型变成工作助手时，幻觉才成了需要解决的问题。人们需要引导大语言模型助手往正确的方向去做梦。也许检索增强生成（RAG, Retrieval-Augmented Generation）是个方向，但是它并非唯一方向。

## 提示词工程视角的问题视角

> 问题其实就是你期望的东西和你体验的东西之间的差别。

所以我们可以看出，所谓提示词工程，就是在理解大语言模型基本原理的基础上，尝试设计一个文本序列，让大语言模型产生更好的结果的一系列活动。

其核心是准确描述问题，限制产生的回应。膜拜提示词工程、背下提示词模板，都没什么意义。提示词会随不同的大语言模型而变，但是规则始终如一。

![neon-god](https://proxy-prod.omnivore-image-cache.app/0x0,so3BeCm7sJ4ru3WZK28l-fZdx_9lshhtwe_62ciQE2pU/https://cdn.sspai.com/2023/12/31/article/cb99c1fbc7e2ed2db62973229a410c4a?imageView2/2/w/1120/q/40/interlace/1/ignore-error/1)

更普适的技能，其实是问题表述——即识别、分析和界定问题。这是人类自古钻研的领域，早就有了很好的教材：[《你的灯亮着吗？》](https://book.douban.com/subject/25772550/)[《学会提问》](https://book.douban.com/subject/20428922/)[《怎样解题》](https://book.douban.com/subject/34900781/)，等等等等。

今年 6 月的《哈佛商业评论》网站上有一篇文章[《AI 提示词工程不是未来》](https://sspai.com/link?target=https%3A%2F%2Fhbr.org%2F2023%2F06%2Fai-prompt-engineering-isnt-the-future)，作者是伦敦国王学院的 [Oguz A. Acar](https://sspai.com/link?target=https%3A%2F%2Fwww.kcl.ac.uk%2Fpeople%2Foguz-a-acar)。他认为，在使用大语言模型时，应该从规划问题的角度来构建提示词。这包括几个步骤：问题识别、问题分解、问题重构、约束设计。

按照 [Gerald M. Weinberg](https://sspai.com/link?target=https%3A%2F%2Fen.wikipedia.org%2Fwiki%2FGerald%5FWeinberg) 的定义，问题是现状和理想之间的差距。描述理想中的目标，再描述现状，我们就得到了问题。有些问题简单具体，有些问题则宽泛得多。

对于宽泛的问题，就需要分解。澄清问题的本质和范围，将问题拆分成更小的可管理的子问题，尤其是当问题涉及多方面因素时更该如此。无论是「我怎样才能瘦 20 斤」还是「我怎样才能财务自由」，都不会有简单答案。

有时候，看待问题的角度决定了解决方案的数量和质量，《你的灯还亮着吗》第一章就提供了一个好例子。问题重构让我们在收集信息的过程中持续回顾问题，看看是否有其它看待问题的角度，甚至考虑最初问题的定义是否有偏差。

最后，我们要定义解决方案的输入、过程和输出来限制问题的边界，这会让大语言模型返回更合适的答案。提示词工程中的「Few-Shot Prompting」方法就是一种典型的约束设计方式，它通过举例来让大语言模型理解任务的要求。

没有明确定义的问题，即使是最复杂的提示词也无法得到优秀的结果。而若问题是定义清晰的，提示词在语言上的细微差别，可能并不那么重要。

我觉得，和大语言模型讨论并解决问题的过程和在论坛上求助有些相似，适用于技术论坛的提问规则也适用于它。所以，《提问的智慧》这篇已经有二十年历史的文章依然很有价值。这里是[英文版](https://sspai.com/link?target=http%3A%2F%2Fwww.catb.org%2F~esr%2Ffaqs%2Fsmart-questions.html)，这里是翻译得很好的[中文版](https://sspai.com/link?target=https%3A%2F%2Fgithub.com%2Fryanhanwu%2FHow-To-Ask-Questions-The-Smart-Way%2Fblob%2Fmain%2FREADME-zh%5FCN.md)。

一句话总结：尝试明确描述问题，一次把问题和想要的解决方案说清楚。

## 更远的未来

今天的企业已经开始部署和使用自己的大语言模型，例如 [Amazon Web Services](https://sspai.com/link?target=https%3A%2F%2Faws.amazon.com%2F) 的 [Amazon Q](https://sspai.com/link?target=https%3A%2F%2Faws.amazon.com%2Fcn%2Fblogs%2Faws%2Fintroducing-amazon-q-a-new-generative-ai-powered-assistant-preview%2F)。企业操纵大语言模型的两种主要方法是微调和 RAG；微调改变了大语言模型的行为和响应，RAG 则通过可信的上下文来补充用户的输入。

也许几年后，我们也可以用类似的方式，把自己的私人大语言模型变成随身分身。也许将来算力、算法、真实世界理解和人机交互等等都会有重大突破，我们无需提示词工程也能获得满意的解决方案。

![sunset](https://proxy-prod.omnivore-image-cache.app/0x0,sjQS9wQBBRoaZU_IX6yDu9jwc0ONbPavVKkzhY162VU8/https://cdn.sspai.com/2023/12/31/article/3632ee50a45091ef36def275b52999d5?imageView2/2/w/1120/q/40/interlace/1/ignore-error/1)

但是基本规则不会变。2023 年 5 月 25 日，李彦宏在中关村论坛做了一场主题演讲：[《大模型将改变世界》](https://new.qq.com/rain/a/20230527A07Q1800)。他说，「10 年后，全世界有 50% 的工作会是提示词工程。」

时间会证明这句话是否正确。不过我觉得，他的下一句话才更关键：

「提出问题比解决问题更重要。」

作者注：本文图片除截图外，皆由 DALL·E 3 生成。

\> 下载少数派 [客户端](https://sspai.com/page/client)、关注 [少数派小红书](https://sspai.com/link?target=https%3A%2F%2Fwww.xiaohongshu.com%2Fuser%2Fprofile%2F63f5d65d000000001001d8d4)，感受精彩数字生活 🍃

\> 实用、好用的[正版软件](https://sspai.com/mall)，少数派为你呈现🚀

---

