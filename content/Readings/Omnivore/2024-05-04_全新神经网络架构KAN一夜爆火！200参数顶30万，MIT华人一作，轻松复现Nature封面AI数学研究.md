---
id: 46d4ab33-d09a-405e-9acc-4a037242fc69
author: |
  关注前沿科技
date_saved: 2024-05-04 01:25:00
date_published: 2024-05-01 16:36:00
draft: true
---

# 全新神经网络架构KAN一夜爆火！200参数顶30万，MIT华人一作，轻松复现Nature封面AI数学研究
#Omnivore

[Read on Omnivore](https://omnivore.app/me/https-mp-weixin-qq-com-s-5-wfjmp-jvtaofe-g-dx-fq-9-a-dw-18f420ff76e)

[Read Original](https://mp.weixin.qq.com/s/5WFJMPJvtaofeGDxFQ9aDw)

date_saved: 2024-05-04 01:25:00

date_published: 2024-05-01 16:36:00

--- 

# Full Content: 

![cover_image](https://proxy-prod.omnivore-image-cache.app/0x0,swRxTtwlo8L72Ch-d8RbQYFMCHS0Uy2R4pLBH6Na-Njo/https://mmbiz.qpic.cn/mmbiz_jpg/YicUhk5aAGtC8hERa3N5OZw5cT1eHzRhVwZDYIqfzDRGw7vvdMCRkQuuCPyBxu3xStSQfHFmLghSuDTmHicVe1dg/0?wx_fmt=jpeg) 

 关注前沿科技  量子位 _2024-05-02 04:36_ _北京_ 

##### 白交 衡宇 发自 凹非寺  
量子位 | 公众号 QbitAI

**一种全新的神经网络架构KAN，诞生了！**

与传统的MLP架构截然不同，且能用更少的参数在数学、物理问题上取得更高精度。

![图片](https://proxy-prod.omnivore-image-cache.app/0x0,sz_OCzbZf55Xv4EDc3VpAukpFdrWySvjkFRlHfCqEKTo/https://mmbiz.qpic.cn/mmbiz_gif/YicUhk5aAGtC8hERa3N5OZw5cT1eHzRhViavtyI794V2Yddy2IdiaIFSFNJP3yccwLSzglkN5NJ5rLlxJ5AxhMUhg/640?wx_fmt=gif&from=appmsg)

比如，200个参数的KANs，就能复现DeepMind用30万参数的MLPs发现数学定理研究。

不仅准确性更高，并且还发现了新的公式。要知道后者可是登上Nature封面的研究啊\~

![图片](https://proxy-prod.omnivore-image-cache.app/0x0,srPwAJm7unzoYdfQ22HyRLQar2xlV5E7AEPcN3yNwUhk/https://mmbiz.qpic.cn/mmbiz_jpg/YicUhk5aAGtC8hERa3N5OZw5cT1eHzRhViasthvRicjLSxW2fAszGnF5j20f7iaibXmNprHWNVt6MJiaHGTfVylicseSw/640?wx_fmt=jpeg&from=appmsg)

在函数拟合、偏微分方程求解，甚至处理凝聚态物理方面的任务都比MLP效果要好。

而在大模型问题的解决上，KAN天然就能规避掉灾难性遗忘问题，并且注入人类的习惯偏差或领域知识非常容易。

来自MIT、加州理工学院、东北大学等团队的研究一出，瞬间引爆一整个科技圈：**Yes We KAN！**

![图片](https://proxy-prod.omnivore-image-cache.app/0x0,s6buhX_uSZh4nI0qDzUL4lftQ97lIsAp0FXyuFUIQKt0/https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtC8hERa3N5OZw5cT1eHzRhVYk7PO0iamkWPRLResiaEpZdmQZ5aKHD7PwAZ34xZgOsS9T5U8uSyHmOA/640?wx_fmt=png&from=appmsg)  
![图片](https://proxy-prod.omnivore-image-cache.app/0x0,sQlK3M2l_fDYW5GqIGIzoUBCKM7IwCSaRTNYMWd8i-Iw/https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtC8hERa3N5OZw5cT1eHzRhV1ibQic6tfibSeU2Dyk0IuNfpIicLiaBeKRpDl09xSfB8chCa6CYGdcVxNkA/640?wx_fmt=png&from=appmsg)

甚至直接引出关于**能否替代掉Transformer的MLP层**的探讨，有人已经准备开始尝试……

![图片](https://proxy-prod.omnivore-image-cache.app/0x0,sEsR7TKBRZ1D2jlLJLT7M2qaHOiOrjDXbw1-c6FEtWPM/https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtC8hERa3N5OZw5cT1eHzRhVia8pBKwBycvERibYBypkGqUxVqicb0WqFEwQLtTfGMj9owlQzgRLbGmog/640?wx_fmt=png&from=appmsg)  
![图片](https://proxy-prod.omnivore-image-cache.app/0x0,sqnzB06kifQ91VXYQ1L6g4rEMxsFNg4fVw-eGp152Bxg/https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtC8hERa3N5OZw5cT1eHzRhVmtSrpnDticfLdF6BUrROwgmtyXMdJTS5nt8kgzm5YHEHsZ3hedwicZVA/640?wx_fmt=png&from=appmsg)

有网友表示：**这看起来像是机器学习的下一步**。

> 让机器学习每个特定神经元的最佳激活，而不是由我们人类决定使用什么激活函数。

![图片](https://proxy-prod.omnivore-image-cache.app/0x0,scHpP7fBG4gbYrmz8hk2DKici_oP0BCm_plvwJuJXZeE/https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtC8hERa3N5OZw5cT1eHzRhV7D32bj9jmJic9kiaXWB0n17CnaeB8XkoZ3hPGVk3yqbGhPtUtDmAJNBg/640?wx_fmt=png&from=appmsg)

还有人表示：可能正处于某些历史发展的中间。

![图片](https://proxy-prod.omnivore-image-cache.app/0x0,soZ-r3SmAK93pJO_uwNyupOaohdCW4z1Soh0cPJM7jzs/https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtC8hERa3N5OZw5cT1eHzRhVs8wU7TbRUJE9lIu4JKCNdrh6LbicicUaRLia2veXYrSJ5W197fSxAGoIw/640?wx_fmt=png&from=appmsg)

GitHub上也已经开源，也就短短两三天时间就收获1.1kStar。

![图片](https://proxy-prod.omnivore-image-cache.app/0x0,sY-Kq0Qg7mtml9TzmYJPY20uuFP2FVNj0p-kN3wgwzeg/https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtC8hERa3N5OZw5cT1eHzRhVAZAtRfsskYNlvkHtwLWvXia6tORmYaHwGL9l22Oia09K0DP8FPI2GxyQ/640?wx_fmt=png&from=appmsg)

## 对MLP“进行一个简单的更改”

跟MLP最大、也是最为直观的不同就是，MLP激活函数是在神经元上，而KAN把可学习的激活函数放在权重上。

![图片](https://proxy-prod.omnivore-image-cache.app/0x0,svqUxUHJGhjVsUbNHuR7VWINhLwGCQb338S9HL-yLMbQ/https://mmbiz.qpic.cn/mmbiz_jpg/YicUhk5aAGtC8hERa3N5OZw5cT1eHzRhVhVFDwagbEgXU18MDPWzIYibtw4bsLqz3wt9r2vujIG2N6bOuZDO7kIA/640?wx_fmt=jpeg&from=appmsg)

在作者看来，这是一个“简单的更改”。

![图片](https://proxy-prod.omnivore-image-cache.app/0x0,sg4cNeACPqo8LSnbo_VhnWhIlcEVDOG6Zh2iTm7u2jF4/https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtC8hERa3N5OZw5cT1eHzRhVX4RVfxffsEKWz4FDeSBwcZ65IJoAQrzDTWqEfhicwoyicGRyebmZPetg/640?wx_fmt=png&from=appmsg)

从数学定理方面来看，MLP的灵感来自于通用近似定理，即对于任意一个连续函数，都可以用一个足够深的神经网络来近似。

而KAN则是来自于 Kolmogorov-Arnold 表示定理 (KART)，每个多元连续函数都可以表示为单变量连续函数的两层嵌套叠加。

![图片](https://proxy-prod.omnivore-image-cache.app/0x0,syPGk_YICICI_krJTqXoLAc2o_JTsxBQqK7PNVxnNiHI/https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtC8hERa3N5OZw5cT1eHzRhVcxmdkSwS6rJDye3fBz5GibL6EslnhMclBFykmkopcGtU2klFGhvfELw/640?wx_fmt=png&from=appmsg)  

KAN的名字也由此而来。

正是受到这一定理的启发，研究人员用神经网络将Kolmogorov-Arnold 表示参数化。

为了纪念两位伟大的已故数学家Andrey Kolmogorov和Vladimir Arnold，我们称其为科尔莫格罗夫-阿诺德网络（KANs）。

![图片](https://proxy-prod.omnivore-image-cache.app/0x0,s78Iat-R8sqOL5_RbiJoiO3KZUKMIkckgB5fuBrF7Oyw/https://mmbiz.qpic.cn/mmbiz_jpg/YicUhk5aAGtC8hERa3N5OZw5cT1eHzRhVVxBgD3OfibBb8uvDwJhBCw01HfOuH1uMg0Nt3seh0ZO7XG3GbSbchxQ/640?wx_fmt=jpeg&from=appmsg)

而从算法层面上看，MLPs 在神经元上具有（通常是固定的）激活函数，而 KANs 在权重上具有（可学习的）激活函数。这些一维激活函数被参数化为样条曲线。

在实际应用过程中，KAN可以直观地可视化，提供MLP无法提供的可解释性和交互性。

![图片](https://proxy-prod.omnivore-image-cache.app/0x0,s6S9hhQZUD_c3GexR_DS04T03w431zHXrio_KZD-Zl9I/https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtC8hERa3N5OZw5cT1eHzRhVWweWX6gdm9OXjaN6HuKDWslC8RicfRRcCq3iaSqL422ylyVFXKuUGqwQ/640?wx_fmt=png&from=appmsg)

不过，KAN的缺点就是训练速度较慢。

对于训练速度慢的问题，MIT博士生一作Ziming Liu解释道，主要有两个方面的原因。

一个是技术原因，可学习的激活函数评估成本比固定激活函数成本更高。

另一个则是主观原因，因为体内物理学家属性抑制程序员的个性，因此没有去尝试优化效率。

![图片](https://proxy-prod.omnivore-image-cache.app/0x0,scuVY3067YJMY3efbICG6iRdj04o0uGACYGWm4XnDL1w/https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtC8hERa3N5OZw5cT1eHzRhV1uNX8Xa4fztVjTABictZtuMVGz6n0k7Vf9oCicicOa8leOiaJbxAYEyscw/640?wx_fmt=png&from=appmsg)

对于是否能适配Transformer，他表示：暂时不知道如何做到这一点。

![图片](https://proxy-prod.omnivore-image-cache.app/0x0,sAFhC4kW_NqfX51AmS4daz86RSWt1HiItb89oaslPIdg/https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtC8hERa3N5OZw5cT1eHzRhVE0Nfs8HdNIm4ria1GeHuHBwLm6dvkusTus0XSLRa3O10tpE7icP0Nmrw/640?wx_fmt=png&from=appmsg)

以及对GPU友好吗？他表示：还没有，正在努力中。

![图片](https://proxy-prod.omnivore-image-cache.app/0x0,sWauPixLrdYKrRNzsFmt3fzEQYqHFLkNx92Ww0fBCbTU/https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtC8hERa3N5OZw5cT1eHzRhVicvBR2IcHRkOl8SSVib2coxh1CtqA2Mxiadm68004uHO6K1QWocrmnh0Q/640?wx_fmt=png&from=appmsg)

## 天然能解决大模型灾难性遗忘

再来看看KAN的具体实现效果。

**神经缩放规律**：KAN 的缩放速度比 MLP 快得多。除了数学上以Kolmogorov-Arnold 表示定理为基础，KAN缩放指数也可以通过经验来实现。

![图片](https://proxy-prod.omnivore-image-cache.app/0x0,sjQObRnKdJCEYvsKkvPc6EfqUCm-b5nyXUxw6Q8vO7x4/https://mmbiz.qpic.cn/mmbiz_jpg/YicUhk5aAGtC8hERa3N5OZw5cT1eHzRhVOmMUcRXMClO8cT6NBUGianBia6bLdRZXZD7SCZBMvol1wnicr3S4YK5Ng/640?wx_fmt=jpeg&from=appmsg)

在**函数拟合**方面，KAN比MLP更准确。

![图片](https://proxy-prod.omnivore-image-cache.app/0x0,ssSyY4gSVHs8BsEkt-hs-zj3X1uYa61OPS0MbXmEvGvM/https://mmbiz.qpic.cn/mmbiz_jpg/YicUhk5aAGtC8hERa3N5OZw5cT1eHzRhV8gurwuHCHiaKphcjDWTojJvAlKV8aTkV1wARLdx26vqFdricOMz2zibOA/640?wx_fmt=jpeg&from=appmsg)

而在**偏微分方程求解**，比如求解泊松方程，KAN比MLP更准确。

![图片](https://proxy-prod.omnivore-image-cache.app/0x0,suZNsleJIMv54pk6p7Wn3M45yuSwaJwOwIFl505hnBUc/https://mmbiz.qpic.cn/mmbiz_jpg/YicUhk5aAGtC8hERa3N5OZw5cT1eHzRhVebGK4cL8FRibU6g29tRTDAlNmGzicX5ssYJRnXTXow3SZraLSpOvh5zg/640?wx_fmt=jpeg&from=appmsg)

研究人员还有个意外发现，就是KAN不会像MLP那样容易**灾难性遗忘**，它天然就可以规避这个缺陷。

好好好，大模型的遗忘问题从源头就能解决。

![图片](https://proxy-prod.omnivore-image-cache.app/0x0,sgSYnu5MhspuSTNcqrfWhiYrQrM4JTVobQthNO97rvnY/https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtC8hERa3N5OZw5cT1eHzRhVS7PadteoiaGDuJZt026gGPMSjlAj4EGmereACfUBEh2GV3pibjOYyycw/640?wx_fmt=png&from=appmsg)

在可解释方面，KAN能通过符号公式揭示合成数据集的组成结构和变量依赖性。

![图片](https://proxy-prod.omnivore-image-cache.app/0x0,si2whVQ1QR9XVcjd-gh3hJcVK-y7rCVGmRj-BSWoQ1lc/https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtC8hERa3N5OZw5cT1eHzRhVvwX199K59BAaINelJfnome9BsNOhVEyNPfibuJrmagm2QUIdnWbDhhA/640?wx_fmt=png&from=appmsg)  

人类用户可以与 KANs 交互，使其更具可解释性。在 KAN 中注入人类的归纳偏差或领域知识非常容易。

![图片](https://proxy-prod.omnivore-image-cache.app/0x0,sD1h-H9CIaBOjHemMeL2fNC8y9uZ-Ys5m91qEaqMwP5U/https://mmbiz.qpic.cn/mmbiz_jpg/YicUhk5aAGtC8hERa3N5OZw5cT1eHzRhVAKNf4NJyNevlCTTbsCeFNRKicj0ZtYE1XxVOT3UpHlwbibOon0YAPFEg/640?wx_fmt=jpeg&from=appmsg)

研究人员利用KANs还重新复现了DeepMind当年登上Nature的结果，并且还找到了Knot理论中新的公式，并以无监督的方式发现了新的结不变式关系。

![图片](https://proxy-prod.omnivore-image-cache.app/0x0,s1FmNaTbbQTy4jjJqK9CDm7l77IASM0bMSMOQZNlPlm8/https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtC8hERa3N5OZw5cT1eHzRhVNeqEJjyx47PPVVCVDfEzSw7YlnibEMUicOZS1avn3RV5kUx6TWad5ZCg/640?wx_fmt=png&from=appmsg)

###### **![图片](https://proxy-prod.omnivore-image-cache.app/0x0,sObx537LCM9ocklrgxS8zytLWqETru4fdy9yDanqrYl4/https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtC8hERa3N5OZw5cT1eHzRhVues0DFTSuqQCFrribpE1L96YE7m4Sic90Vrqb7oJVvKiandSzvuAiajBcQ/640?wx_fmt=png&from=appmsg)**

###### **△**DeepMind登Nature研究成果

Deepmind的MLP大约300000 个参数，而KAN大约只有200 个参数。KAN 可以立即进行解释，而 MLP 则需要进行特征归因的后期分析。并且准确性也更高。

对于计算要求，团队表示论文中的所有例子都可以在单个CPU上10分钟内重现。

虽然KAN所能处理的问题规模比许多机器学习任务要小，但对于科学相关任务来说就刚刚好。

比如研究凝固态物理中的一种相变：安德森局域化。

![图片](https://proxy-prod.omnivore-image-cache.app/0x0,sSqd540e0-J1pb7_Z4lsMEHzjlIg-mYvLkKgcLipoTqk/https://mmbiz.qpic.cn/mmbiz_jpg/YicUhk5aAGtC8hERa3N5OZw5cT1eHzRhVRCkM50oLOfD5IdbyoZrCOdy0POQVZkjA64eMhM6DnmviauyibicaRibdnA/640?wx_fmt=jpeg&from=appmsg)

好了，那么KAN是否会取代Transformer中的MLP层呢？

有网友表示，这取决于两个因素。

一点是学习算法，如 SGD、AdamW、Sophia 等—能否找到适合 KANs 参数的局部最小值？

另一点则是能否在GPU上高效地实现KANs层，最好能比MLPs跟快。

![图片](https://proxy-prod.omnivore-image-cache.app/0x0,s-ELM5rD2uS8T39rDxmdHVhCpahDHz7oXGzuv7ZSSkF4/https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtC8hERa3N5OZw5cT1eHzRhVAy75q7JyrMlqSBx8sox94dd69LK8Uhevxu0BN721En2YdKIEtJxWgw/640?wx_fmt=png&from=appmsg)

最后，论文中还贴心的给出了“何时该选用KAN？”的决策树。

![图片](https://proxy-prod.omnivore-image-cache.app/0x0,s6uYU5CViJ0S3rHUO5ZS_f-VoGhn2xIB4Bepb2QtVlSk/https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtC8hERa3N5OZw5cT1eHzRhVia2oWjaYLRtuYcAKR9zDiaE3LNlNXyey6XuEU5RyRibJruQicgKpQrKgZg/640?wx_fmt=png&from=appmsg)

那么，你会开始尝试用KAN吗？还是让子弹再飞一会儿\~

项目链接：  
https://kindxiaoming.github.io/pykan/  
论文链接：  
https://arxiv.org/abs/2404.19756  
参考链接：  
\[1\]https://twitter.com/ZimingLiu11/status/1785483967719981538  
\[2\]https://twitter.com/AnthropicAI/status/1785701418546180326

— **完** —

**点这里👇关注我，记得标星哦～**

**一键三连「分享」、「点赞」和「在看」**

**科技前沿进展日日相见 \~** 

![图片](https://proxy-prod.omnivore-image-cache.app/0x0,szwoYh5XQirpIcWEJ__WUUlvmSKwVoepjpKctWSCSs_E/https://mmbiz.qpic.cn/mmbiz_svg/g9RQicMD01M0tYoRQT2cMQRmPS5ZDyrrfzeksiay90KaDzlGBH61icqHxmgFKfvfXtVuwTHV740CDLAaXU1LIfZyoJEpYKcRIiaE/640?wx_fmt=svg)

---

