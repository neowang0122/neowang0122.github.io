---
id: 5f4e091d-76af-4666-a3d5-aafcad885357
title: |
  总结李飞飞教授讲义中的神经网络调参技巧
author: |
  泉林泉林​关注他
date_saved: 2023-07-04 07:56:46
date_published: 2019-01-07 05:54:00
draft: true
---

# 总结李飞飞教授讲义中的神经网络调参技巧
#Omnivore

[Read on Omnivore](https://omnivore.app/me/-18920bd50f6)

[Read Original](https://zhuanlan.zhihu.com/p/51678880)

date_saved: 2023-07-04 07:56:46

date_published: 2019-01-07 05:54:00

--- 

# Full Content: 

## 总结李飞飞教授讲义中的神经网络调参技巧

总结了李飞飞教授的讲义，稍微结合自己的一些经验（也就是厚着脸皮说一下结合了自己的经验了，其实自己的那些经验，都在教授的讲义中了。。。。）。

**数据预处理：**

1. 零均值（zero-centered），将数据的均值变成零。所有数据整体减去本身的均值就行了。
2. 归一化（normalize），把数据各个向量的方差改成1\. （1,2两点，在BN层中其实也是这样的思想了）
3. 去相关性（decorrelate），将数据各个分量设置为互相独立的。协方差矩阵是对角阵。
4. 白化（whiten），协方差矩阵是单位阵，零均值，方差为1.

这些操作在神经网络中比较常见。有些层的设计也是基于上面的一些思想来的。

**少量数据的验证：**

先使用少量数据，确保网络能够在这些很少的数据集上完全拟合。如果是自己写的代码，可以验证有没有错误，结构设计有没有缺陷。

**学习率：**

这个调整起来相当看经验。。。。。下面总结的几点也不是绝对的。

1. 如果loss 没有下降：学习率可能过低或者过高。过低是因为每次更新的太小了，总在很小的地方波动，网络基本无法训练。过高是因为在某个最低点附近震动，无法进入那个最低点。（此处非常感谢 @赵英俊 指出问题！）。
2. 如果loss 爆炸了：学习率过高
3. 如果loss出现了NaN：一个可能是学习率过高，另外可能是自己写的哪里有bug。（有时候有一些奇怪的情况，非常偶尔出现NaN，就是查不出哪里错了，而且不能复现NaN，后面的实验都是正常的。这个问题我也不清楚，难道是GPU过热了23333。。。）（NaN的情况有@ValK的分析，已经推荐答案，非常感谢@ValK的分析！）

一般而言会用验证集来调整最好的学习率。

训练的时候，如果loss 大于三倍的初始loss，基本就是要炸了。

另外有一些训练技巧，比如warm up，会先用一个比较小的学习率训练一下网络，然后再开始正式的训练。

**loss 曲线的分析：**

1. 如果loss 曲线下降的接近线性，说明学习率比较小。因为一般而言，loss 曲线下降都是逐渐缓和的。
2. 如果曲线抖动的太厉害，可能是batch size 过小。可以增大batch size。
3. 如果训练和验证集上loss曲线距离太大，说明过拟合比较严重，可以增大正则项的权重。
4. 如果训练和验证集上loss曲线没有距离，说明网络 capacity 不够，需要增加。（我的理解是，网络对于训练数据处于欠拟合状态，性能还有提升的空间，训练集上提升的同时，验证集合上也可能会有提升，然后随着网络逐渐能够拟合直至过拟合，验证集上的性能会逐渐升高然后降低。我们希望找到的是验证集上最好的性能，这时候通常训练集上的性能都会比验证集上高一些。当然，如果loss训练和验证都是0，只能说明任务太简单，是极端情况了。）

**其他的点**

1. 控制 梯度/权重的比值：大约在 0.01\~0.001左右（虽然是个病句吧，但是懂啥意思就行）。
2. 可以可视化出来第一层的权重，如果看起来是一些噪声，加强正则项。

---

