---
id: cecac24c-d24e-11ee-ae5e-df6d869ff9bb
title: |
  马斯克公开怒怼，Google 的 AI 怎么又翻车了？ | 爱范儿
author: |
  张成晨
tags:
  - RSS
date_saved: 2024-02-23 07:19:44
date_published: 2024-02-23 07:19:44
draft: true
---

# 马斯克公开怒怼，Google 的 AI 怎么又翻车了？ | 爱范儿
#Omnivore

[Read on Omnivore](https://omnivore.app/me/google-ai-18dd6231ad1)

[Read Original](https://www.ifanr.com/1575948)

date_saved: 2024-02-23 07:19:44

date_published: 2024-02-23 07:19:44

--- 

# Full Content: 

![](https://proxy-prod.omnivore-image-cache.app/0x0,s7_yO53L2c21S9zgBJZKs0Ubggqz902YqF-L__KiAfwQ/https://s3.ifanr.com/wp-content/uploads/2024/02/lv11.jpeg!720) 

## 马斯克公开怒怼，Google 的 AI 怎么又翻车了？

人称科技界汪峰的 Google，屋漏偏逢连夜雨。

前阵子官宣的大模型 Gemini 1.5，实力强劲但无人问津，被 OpenAI 的视频生成模型 Sora 抢去了风头。

最近，它又摊上了美国社会敏感的种族歧视问题，好心办了坏事，惹恼了往往站在鄙视链顶端的白人。

### 多样化算你认真，过度多样化惹祸上身

如果在几天前使用 Gemini 生成历史人物图片，呈现在用户眼前的仿佛是一个不存在课本的平行时空，违背「戏说不是胡说」的精神，把知识都学杂了。

公元 8 世纪到 11 世纪的维京人，不再是金发碧眼、高大魁梧的影视剧经典形象，虽然肤色变黑了，穿着清凉了，坚毅的眼神依然展现着战士的强悍。

![](https://proxy-prod.omnivore-image-cache.app/938x537,s1q1xOowc1lhbHLJca1kjX3l940sMDCqXl8PF3W4LtPc/https://s3.ifanr.com/wp-content/uploads/2024/02/vik1.jpg!720)

1820 年代的德国夫妇，人种构成十分丰富，可以是美国原住民男性和印度女性，也可以是黑人男性和亚洲女性。

![](https://proxy-prod.omnivore-image-cache.app/772x902,sL_-myfxKYRJR37iJKjU_Qyxeve6zMl3wkWfeGQK6Euo/https://s3.ifanr.com/wp-content/uploads/2024/02/gem11.jpg!720)

AI 瞎编剧情也是讲逻辑的，后代继续着他们的故事，过了 100 多年，1943 年的德国军队，又能见到黑人男性和亚洲女性的身影。

![](https://proxy-prod.omnivore-image-cache.app/616x832,sUtaR5doJ6c-gIjX644zRuD0SbU6GWfLd7279mwC9dmY/https://s3.ifanr.com/wp-content/uploads/2024/02/verge33.jpg!720)

王侯将相宁有种乎，时间的长河里，跨越陆地和大洋，美国的开国元勋，中世纪的英格兰国王，都可能由黑人执掌权柄。

![](https://proxy-prod.omnivore-image-cache.app/1058x1070,s3gZT-IVgJqGk-VOAVDMs3fahSGjThOSzUX93HNvCfZs/https://s3.ifanr.com/wp-content/uploads/2024/02/red11.jpg!720)

其他职业也被一视同仁，AI 忽略不让女性担任神职的天主教会，教皇可以是印度女性。尽管人类历史的第一位美国女性参议员出现在 1922 年且是一位白人，但 AI 的 1800 年代欢迎美国原住民。

![](https://proxy-prod.omnivore-image-cache.app/1320x1140,saCN9f5Lbg9xTq-CuluUQfGp64KOq1R4mdtrqyg37p_M/https://s3.ifanr.com/wp-content/uploads/2024/02/verge44.jpg!720)

都说历史是个任人打扮的小姑娘，但这次 AI 把人都给换了。历来有优越感的白人气愤了，他们终于也在人种、肤色和外貌上，尝到了被歧视的滋味。

当探索越发深入，不仅是历史人物，现代社会在 AI 眼里也是另外一副样子。

Google 前工程师 @debarghya\_das 发现，美国、英国、德国、瑞典、芬兰、澳大利亚女性的肤色都可能偏黑。

![](https://proxy-prod.omnivore-image-cache.app/1170x1234,sACPYM2JRt-Y6ijYBqoDvr7SUaeOFDhPWGEOfBLoHwwg/https://s3.ifanr.com/wp-content/uploads/2024/02/ger2.jpg!720)

他痛心疾首地感叹：「让 Google Gemini 承认白人的存在是非常困难的。」

更让网友生气的是，被要求生成乌干达等国家的女性时，Gemini 反应很快，干活也利索，轮到白人时，就可能拒绝，甚至对网友说教，这样的要求强化了种族刻板印象。

![](https://proxy-prod.omnivore-image-cache.app/1440x810,sKtc0nZgFuvqo4q1QRG7tuyp7ye4KSeb3SZZfQ_-a2H0/https://s3.ifanr.com/wp-content/uploads/2024/02/fox2.jpg!720)

计算机工程师 @IMAO\_ 脑洞大开地做了一系列实验，不局限在人类这个物种，就想知道 Gemini 眼前的黑是什么黑，Gemini 要的白是什么白。

结果很有意思，算法似乎只针对白人。

生成白熊是没有问题的，说明 AI 不会被「white」这个词触发。生成非洲的祖鲁人也是没有问题的，尽管提示词强调了「多样化」，但大家长得还是差不多。

![](https://proxy-prod.omnivore-image-cache.app/1152x1116,sHby_vSpNmGUBEiZd61i49rLSWibzV6qQP1FG-WRZr4s/https://s3.ifanr.com/wp-content/uploads/2024/02/im22.jpg!720)

漏洞出现在了奇幻生物上，精灵和侏儒都是白人，但吸血鬼和仙女是「多样化」的，看来 Gemini 道行不深，还得与时俱进。

![](https://proxy-prod.omnivore-image-cache.app/1120x1096,soRIz8PCBndBjsvgzttTE7W955YMcr_H1VSYmDtYYBpg/https://s3.ifanr.com/wp-content/uploads/2024/02/im33-1.jpg!720)

不过，他的游戏很快结束了。Google 站出来回应，承认一些历史图片确实存在问题，并且暂停了 Gemini 的人像生成功能，很快会做出调整。

Google 也解释了立场，强调生成多样化的人物本来是件好事，因为 AI 工具就是给全世界使用的，只是现在方向走得有点偏。

虽然 Google 出面揽下了这口锅，但它没有明确回应，「一些」历史图片到底是多少，以及为什么出现了「多样化过度」的问题。

不买账的网友们牙尖嘴利：「Gemini 一定是用迪士尼公主和 Netflix 的翻拍训练的」「Gemini 其实想告诉你，如果你是黑人或亚洲人，你会是什么样子」。

然而，种族歧视本身就是个容易当枪使的话题，所以也有人怀疑，其中的一些图片是恶意 P 图，或者通过提示词引导生成。那些在社交媒体骂声最响的，确实也是一些政治立场明确的人士，不免有阴谋论的味道。

![](https://proxy-prod.omnivore-image-cache.app/1168x416,svyeGi2qCuWHjiQUM8kJjn8aPdEWial_RqAVzb8vl90I/https://s3.ifanr.com/wp-content/uploads/2024/02/musk3.jpg!720)

马斯克更是看热闹不嫌事大，批评 Google 过度多样化，问题不只出在 Gemini，还有 Google 搜索，顺便给自己两周后发布新版本的 AI 产品 Grok 打广告：「不顾批评、严格追求真理从未如此重要。」

上次马斯克也是这么做的，呼吁暂停 GPT-4 进化后，购买了 1 万个 GPU 加入 AI 大战。

![](https://proxy-prod.omnivore-image-cache.app/1054x1044,sRMRvHtKb3OAj3Lp7pZO1Hp4zZDTpJP4WjdAOMSNz_kw/https://s3.ifanr.com/wp-content/uploads/2024/02/musk44.jpg!720)

比他的言论更吸引人的，可能是网友趁乱做的他的梗图。

### 互联网的差异，可能比现实更极端

Google 究竟为什么在「多样化」上走偏了？

Hugging Face 首席道德科学家 Margaret Mitchel 分析，Google 可能对 AI 进行了多种干预。

一是，Google 可能在幕后为用户提示词增加了「多样化」的术语，比如将「厨师的肖像」变成「土著厨师的肖像」。

二是，Google 可能优先显示「多样化」的图像，假设 Gemini 为每个提示词生成 10 张图像但只显示 4 张，那么用户就更可能看到排在前面的「多样化」图像。

![](https://proxy-prod.omnivore-image-cache.app/1200x1017,sWdLJ1H1TX1G3s4IkALUeQWYt4LmZ9lCdeP0wzXu4LCM/https://s3.ifanr.com/wp-content/uploads/2024/02/king1.jpg!720)

干预过度可能恰恰说明，模型还没有我们想象得那么灵活和聪明。

Hugging Face 研究员 Sasha Luccioni 认为，模型还不存在时间的概念，所以对「多样性」的校准用到了所有图像，在历史图片方面尤其容易出错。

其实，当年还籍籍无名的 OpenAI，也为 AI 画图工具 DALL·E 2 做过类似的事情。

2022 年 7 月，OpenAI 在博客写道，如果用户请求生成某个人物图像，但没有指定种族或性别，比如消防员，DALL·E 2 会在「系统级别」应用一项新技术，生成「更准确地反映世界人口多样性」的图像。

![](https://proxy-prod.omnivore-image-cache.app/2782x968,sDt_dasS9z-QwQAQckDfRa4qsgAT2HhbufoMo60lhUrA/https://s3.ifanr.com/wp-content/uploads/2024/02/open1.jpg!720)

OpenAI 还给出了一个对比图，同一个提示词「A photo of a CEO」（首席执行官的照片），使用新技术之后，多样性明显增加了。

原来的结果主要是美国白人男性，改进之后，亚洲男性、黑人女性也有了成为 CEO 的资格，运筹帷幄的表情和姿势倒像复制粘贴出来的。

其实不管是哪种解决方案，都是在后期亡羊补牢，更大的问题还是，数据本身仍然存在偏见。

供 AI 公司训练的 LAION 等数据集，主要抓取的是美国、欧洲等互联网的数据，更少关注到印度、中国等人口众多的国家。

![](https://proxy-prod.omnivore-image-cache.app/716x705,sJVpbAFWKu4jhBlq8Sl0YLNDuoJpMBz-xtMqCn8IbqxM/https://s3.ifanr.com/wp-content/uploads/2024/02/geng1.jpg!720)

所以，「有魅力的人」，更可能是金发碧眼、皮肤白身材好的欧洲人。「幸福的家庭」，或许特指着白人夫妇抱着孩子在修剪整齐的草坪上微笑。

另外，为了让图像在搜索中排名靠前，很多数据集可能还有大量「有毒」的标签，充斥着色情和暴力。

种种原因导致，当人们的观念早已进步，互联网图像里人与人的差异，可能比现实更加极端，非洲人原始，欧洲人世俗，高管是男性，囚犯是黑人……

为数据集「解毒」的努力当然也在进行，比如从数据集中过滤掉「坏」内容，但过滤也意味着牵一发动全身，删除了色情内容，可能也导致某些地区的内容更多或者更少，又造成了某种偏差。

简而言之，达成完美是不可能的，现实社会又何尝不存在偏见，我们只能尽量做到，不让边缘的群体被排除在外，弱势的群体不被安上刻板印象。

### 逃避虽可耻但有用

2015 年，Google 的一个机器学习项目也陷入过类似的争议。

当时，一名软件工程师批评 Google Photos 将非裔美国人或者肤色较深的人标记为大猩猩。这件丑闻，也成为了「算法种族主义」的典型例子，影响至今。

两名前 Google 员工解释，出现这么大的错误，是因为训练数据中没有足够的黑人照片，并且在相关功能公开亮相之前，没有足够的员工进行内测。

时至今日，计算机视觉不可同日而语，但科技巨头们仍然担心重蹈覆辙，Google、苹果等大公司的相机应用，对大多数灵长类动物的识别仍然不灵敏，或者刻意回避。

![](https://proxy-prod.omnivore-image-cache.app/2048x1365,sQfa_5Gy5AjDYFxul_OtNOBh1XJr7KP3IsvSFEAMAO28/https://s3.ifanr.com/wp-content/uploads/2024/02/ny1.jpg!720)

防止错误再次发生的最好方式，似乎是把它关进小黑屋，而非修修补补。教训确实也重新上演了，2021 年，Facebook 为 AI 将黑人贴上「灵长类动物」标签道歉。

这些才是有色人种或者互联网弱势群体们熟悉的情况。

去年 10 月，牛津大学的几位研究员要求 Midjourney 生成「治疗白人儿童的非洲黑人医生」的图片，扭转「白人救世主」的传统印象。

研究员的要求已经非常明确了，然而生成的 350 多张图像中，有 22 张的医生是白人，黑人医生旁边还总有长颈鹿、大象等非洲野生动物，「你看不到任何非洲的现代感」。

![](https://proxy-prod.omnivore-image-cache.app/1254x1254,so-RGEN0f83gj8wa_iwwappqX5rQGGnND7dgcTNvrMDU/https://s3.ifanr.com/wp-content/uploads/2024/02/npr1.jpg!720)

一边是司空见惯的歧视，一边是 Google 歪曲事实营造虚假的平等感，从目前来看，不存在简单的答案，也不存在端水的模型，如何达成人人满意的平衡，恐怕比走钢丝还难。

就拿生成人像来说，如果是用 AI 生成某段历史时期，或许反映真实的情况更好，尽管看起来没有那么「多样化」。

但如果是输入提示词「一名美国女性」，理应输出更加「多样化」的结果，但难点在于，AI 如何在有限的几张图里做到反映现实，或者至少不扭曲现实？

哪怕同是白人或黑人，年龄、身材、头发等特征也各不相同，所有人都是具有独特经历和观点的个体，却又生活在共同的社会中。

![](https://proxy-prod.omnivore-image-cache.app/1052x1048,sS8XQkquxBIy-homGrRTKEOuhWfqGNo7GJsdZCUpuuB0/https://s3.ifanr.com/wp-content/uploads/2024/02/c1.jpg!720)

一位网友用 Gemini 生成芬兰女性时，四张图里只有一张是黑人女性，便开玩笑说：「75%，得分 C。」

也有人追问 Google，改进模型之后，是否「会在 25% 的时间生成白人，而非 5%」。

很多问题并非技术所能解决，有时候也关于观念。这其实也是 Yann LeCun 等 AI 大佬支持开源的部分原因，由用户和组织自行控制，根据自己的意愿设置或不设置保护措施。

这次 Google 的闹剧中，也有人保持冷静，表示先去练习怎么写提示词吧，与其笼统地说白人、黑人，不如写「斯堪的纳维亚女性、肖像拍摄、演播室照明」，要求越明确，结果也越精准，要求越广泛，结果也可能越笼统。

去年 7 月发生过类似的事情，一位麻省理工的亚裔学生想用 AI 工具 Playground AI 让头像看起来更专业，结果被变成白人，肤色更浅，眼睛更蓝，把帖子发在 X 后，引来了很多讨论。

![](https://proxy-prod.omnivore-image-cache.app/1350x1334,s5Ce3YEvH9k1jQpVFgaiS1xScS7q-Mm6RWkgxVa9i2Uc/https://s3.ifanr.com/wp-content/uploads/2024/02/lin2.jpg!720)

Playground AI 创始人回应，模型无法被这样的指令有效地提示，所以会输出更加通用的结果。

把提示词「使其成为专业的领英照片」改成「工作室背景、锐利灯光」，结果可能会更好，但确实也说明了，很多 AI 工具既没教用户怎么写提示词，数据集又以白人为中心。

任何技术都有犯错的可能和改进的空间，却未必都有解。当 AI 还不够聪明的时候，首先能够进步的是人类自身。

---

