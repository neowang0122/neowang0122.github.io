---
id: b36b035c-962f-4743-b939-f126166a6973
title: |
  Next generation reservoir computing | Nature Communications
author: |
  Barbosa, Wendson A. S.
date_saved: 2023-11-12 11:58:21
date_published: 2022-01-10 19:00:00
draft: true
---

# Next generation reservoir computing | Nature Communications
#Omnivore

[Read on Omnivore](https://omnivore.app/me/next-generation-reservoir-computing-nature-communications-18bc4785204)

[Read Original](https://www.nature.com/articles/s41467-021-25801-2)

date_saved: 2023-11-12 11:58:21

date_published: 2022-01-10 19:00:00

--- 

# Full Content: 

## Introduction

A dynamical system evolves in time, with examples including the Earth’s weather system and human-built devices such as unmanned aerial vehicles. One practical goal is to develop models for forecasting their behavior. Recent machine learning (ML) approaches can generate a model using only observed data, but many of these algorithms tend to be data hungry, requiring long observation times and substantial computational resources.

Reservoir computing[1](https://www.nature.com/articles/s41467-021-25801-2#ref-CR1 "Jaeger, H. & Haas, H. Harnessing nonlinearity: predicting chaotic systems and saving energy in wireless communication. Science 304, 78–80 (2004)."),[2](https://www.nature.com/articles/s41467-021-25801-2#ref-CR2 "Maass, W., Natschläger, T. & Markram, H. Real-time computing without stable states: a new framework for neural computation based on perturbations. Neural Comput. 14, 2531–2560 (2002).") is an ML paradigm that is especially well-suited for learning dynamical systems. Even when systems display chaotic[3](https://www.nature.com/articles/s41467-021-25801-2#ref-CR3 "Pathak, J., Lu, Z., Hunt, B. R., Girvan, M. & Ott, E. Using machine learning to replicate chaotic attractors and calculate Lyapunov exponents from data. Chaos 27, 121102 (2017).") or complex spatiotemporal behaviors[4](https://www.nature.com/articles/s41467-021-25801-2#ref-CR4 "Pathak, J., Hunt, B., Girvan, M., Lu, Z. & Ott, E. Model-free prediction of large spatiotemporally chaotic systems from data: a reservoir computing approach. Phys. Rev. Lett. 120, 24102 (2018)."), which are considered the hardest-of-the-hard problems, an optimized reservoir computer (RC) can handle them with ease.

As described in greater detail in the next section, an RC is based on a recurrent artificial neural network with a pool of interconnected neurons—the reservoir, an input layer feeding observed data **X** to the network, and an output layer weighting the network states as shown in Fig. [1](https://www.nature.com/articles/s41467-021-25801-2#Fig1). To avoid the vanishing gradient problem[5](https://www.nature.com/articles/s41467-021-25801-2#ref-CR5 "Bengio, Y., Boulanger-Lewandowski, N. & Pascanu, R. Advances in optimizing recurrent networks. 2013 IEEE International Conference on Acoustics, Speech and Signal Processing, 2013, pp. 8624–8628 
                  https://doi.org/10.1109/ICASSP.2013.6639349
                  
                 (2013).") during training, the RC paradigm randomly assigns the input-layer and reservoir link weights. Only the weights of the output links **W**out are trained via a regularized linear least-squares optimization procedure[6](https://www.nature.com/articles/s41467-021-25801-2#ref-CR6 "Vogel, C. R. Computational Methods for Inverse Problems (Society for Industrial and Applied Mathematics, 2002)."). Importantly, the regularization parameter α is set to prevent overfitting to the training data in a controlled and well understood manner and makes the procedure noise tolerant. RCs perform as well as other ML methods, such as Deep Learning, on dynamical systems tasks but have substantially smaller data set requirements and faster training times[7](https://www.nature.com/articles/s41467-021-25801-2#ref-CR7 "Vlachas, P. R. et al. Backpropagation algorithms and reservoir computing in recurrent neural networks for the forecasting of complex spatiotemporal dynamics. Neural Netw. 126, 191–217 (2020)."),[8](https://www.nature.com/articles/s41467-021-25801-2#ref-CR8 "Bompas, S., Georgeot, B. & Guéry-Odelin, D. Accuracy of neural networks for the simulation of chaotic dynamics: precision of training data vs precision of the algorithm. Chaos 30, 113118 (2020).").

**Fig. 1: A traditional RC is implicit in an NG-RC.**

[![figure 1](https://proxy-prod.omnivore-image-cache.app/685x500,stuPsOCU9jqYE_hZn7f3pHQqhmVYaP6nTc6DVy9PJiPQ/https://media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41467-021-25801-2/MediaObjects/41467_2021_25801_Fig1_HTML.png)](https://www.nature.com/articles/s41467-021-25801-2/figures/1)

(top) A traditional RC processes time-series data associated with a strange attractor (blue, middle left) using an artificial recurrent neural network. The forecasted strange attractor (red, middle right) is a linear weight of the reservoir states. (bottom) The NG-RC performs a forecast using a linear weight of time-delay states (two times shown here) of the time series data and nonlinear functionals of this data (quadratic functional shown here).

Using random matrices in an RC presents problems: many perform well, but others do not all and there is little guidance to select good or bad matrices. Furthermore, there are several RC metaparameters that can greatly affect its performance and require optimization[9](#ref-CR9 "Yperman, J. & Becker, T. Bayesian optimization of hyper-parameters in reservoir computing. Preprint at arXiv:1611.0519 (2016)."),[10](#ref-CR10 "Livi, L., Bianchi, F. M. & Alippi, C. Determination of the edge of criticality in echo state networks through fisher information maximization. IEEE Trans. Neural Netw. Learn. Syst. 29, 706–717 (2018)."),[11](#ref-CR11 "Thiede, L. A. & Parlitz, U. Gradient based hyperparameter optimization in echo state networks. Neural Netw. 115, 23–29 (2019)."),[12](#ref-CR12 "Griffith, A., Pomerance, A. & Gauthier, D. J. Forecasting chaotic systems with very low connectivity reservoir computers. Chaos 29, 123108 (2019)."),[13](https://www.nature.com/articles/s41467-021-25801-2#ref-CR13 "Antonik, P., Marsal, N., Brunner, D. & Rontani, D. Bayesian optimisation of large-scale photonic reservoir computers. Cogn. Comput. 2021, 1–9 (2021)."). Recent work suggests that good matrices and metaparameters can be identified by determining whether the reservoir dynamics _r_ synchronizes in a generalized sense to **X**[14](https://www.nature.com/articles/s41467-021-25801-2#ref-CR14 "Lu, Z., Hunt, B. R. & Ott, E. Attractor reconstruction by machine learning. Chaos 28, 061104 (2018)."),[15](https://www.nature.com/articles/s41467-021-25801-2#ref-CR15 "Platt, J. A., Wong, A. S., Clark, R., Penny, S. G. & Abarbanel, H. D. I. Robust forecasting through generalized synchronization in reservoir computing. Preprint at arXiv:2103.0036 (2021)."), but there are no known design rules for obtaining generalized synchronization.

Recent RC research has identified requirements for realizing a general, universal approximator of dynamical systems. A universal approximator can be realized using an RC with nonlinear activation at nodes in the recurrent network and an output layer (known as the feature vector) that is a weighted linear sum of the network nodes under the weak assumptions that the dynamical system has bounded orbits[16](https://www.nature.com/articles/s41467-021-25801-2#ref-CR16 "Gonon, L. & Ortega, J. P. Reservoir computing universality with stochastic inputs. IEEE Trans. Neural Netw. Learn. Syst. 31, 100–112 (2020).").

Less appreciated is the fact that an RC with linear activation nodes combined with a feature vector that is a weighted sum of nonlinear functions of the reservoir node values is an equivalently powerful universal approximator[16](https://www.nature.com/articles/s41467-021-25801-2#ref-CR16 "Gonon, L. & Ortega, J. P. Reservoir computing universality with stochastic inputs. IEEE Trans. Neural Netw. Learn. Syst. 31, 100–112 (2020)."),[17](https://www.nature.com/articles/s41467-021-25801-2#ref-CR17 "Hart, A. G., Hook, J. L. & Dawes, J. H. P. Echo state networks trained by Tikhonov least squares are L2(μ) approximators of ergodic dynamical systems. Phys. D. Nonlinear Phenom. 421, 132882 (2021)."). Furthermore, such an RC is mathematically identical to a nonlinear vector autoregression (NVAR) machine[18](https://www.nature.com/articles/s41467-021-25801-2#ref-CR18 "Bollt, E. On explaining the surprising success of reservoir computing forecaster of chaos? The universal machine learning dynamical system with contrast to VAR and DMD. Chaos 31, 013108 (2021)."). Here, no reservoir is required: the feature vector of the NVAR consists of _k_ time-delay observations of the dynamical system to be learned and nonlinear functions of these observations, as illustrated in Fig. [1](https://www.nature.com/articles/s41467-021-25801-2#Fig1), a surprising result given the apparent lack of a reservoir!

These results are in the form of an existence proof: There exists an NVAR that can perform equally well as an optimized RC and, in turn, the RC is implicit in an NVAR. Here, we demonstrate that it is easy to design a well-performing NVAR for three challenging RC benchmark problems: (1) forecasting the short-term dynamics; (2) reproducing the long-term climate of a chaotic system (that is, reconstructing the attractors shown in Fig. [1](https://www.nature.com/articles/s41467-021-25801-2#Fig1)); and (3) inferring the behavior of unseen data of a dynamical system.

Predominantly, the recent literature has focused on the first benchmark of short-term forecasting of stochastic processes time-series data[16](https://www.nature.com/articles/s41467-021-25801-2#ref-CR16 "Gonon, L. & Ortega, J. P. Reservoir computing universality with stochastic inputs. IEEE Trans. Neural Netw. Learn. Syst. 31, 100–112 (2020)."), but the importance of high-accuracy forecasting and inference of unseen data cannot be overstated. The NVAR, which we call the next generation RC (NG-RC), displays state-of-the-art performance on these tasks because it is associated with an implicit RC, and uses exceedingly small data sets and side-steps the random and parametric difficulties of directly implementing a traditional RC.

We briefly review traditional RCs and introduce an RC with linear reservoir nodes and a nonlinear output layer. We then introduce the NG-RC and discuss the remaining metaparameters, introduce two model systems we use to showcase the performance of the NG-RC, and present our findings. Finally, we discuss the implications of our work and future directions.

The purpose of an RC illustrated in the top panel of Fig. [1](https://www.nature.com/articles/s41467-021-25801-2#Fig1) is to broadcast input data **X** into the higher-dimensional reservoir network composed of _N_ interconnected nodes and then to combine the resulting reservoir state into an output **Y** that closely matches the desired output **Y**_d_. The strength of the node-to-node connections, represented by the connectivity (or adjacency) matrix **A**, are chosen randomly and kept fixed. The data to be processed **X** is broadcast into the reservoir through the input layer with fixed random coefficients **W**. The reservoir is a dynamic system whose dynamics can be represented by

{{{{{{\\bf{r}}}}}}}\_{i+1}=\\left(1-\\gamma \\right){{{{{{\\bf{r}}}}}}}\_{i}+\\gamma f\\left({{{{{\\bf{A}}}}}}{{{{{{\\bf{r}}}}}}}\_{i}+{{{{{\\bf{W}}}}}}{{{{{{\\bf{X}}}}}}}\_{i}+{{{{{\\bf{b}}}}}}\\right),

 (1)

where {{{{{{\\bf{r}}}}}}}\_{i}={\\left\[{r}\_{1,i},{r}\_{2,i},...,{r}\_{N,i}\\right\]}^{T}\\,is an _N_\-dimensional vector with component _r_ _j,i_ representing the state of the _j_th node at the time _t_ _i_, _γ_ is the decay rate of the nodes, _f_ an activation function applied to each vector component, and **b** is a node bias vector. For simplicity, we choose _γ_ and **b** the same for all nodes. Here, time is discretized at a finite sample time _dt_ and _i_ indicates the _i_th time step so that _dt_ \= _t_ _i_+1\-_t_ _i_. Thus, the notations **r**_i_ and **r**_i_+1 represent the reservoir state in consecutive time steps. The reservoir can also equally well be represented by continuous-time ordinary differential equations that may include the possibility of delays along the network links[19](https://www.nature.com/articles/s41467-021-25801-2#ref-CR19 "Gauthier, D. J. Reservoir computing: harnessing a universal dynamical system. SIAM News 51, 12 (2018).").

The output layer expresses the RC output **Y**_i_+1 as a linear transformation of a feature vector {{\\mathbb{O}}}\_{{{{{{{\\mathrm{total}}}}}}},{i+1}}, constructed from the reservoir state **r**_i_+1, through the relation

{{{{{{\\bf{Y}}}}}}}\_{i+1}={{{{{{\\bf{W}}}}}}}\_{{{{{{{\\mathrm{out}}}}}}}}{{\\mathbb{O}}}\_{{{{{{{\\mathrm{total}}}}}}},i+1},

 (2)

where **W**out is the output weight matrix and the subscript total indicates that it can be composed of constant, linear, and nonlinear terms as explained below. The standard approach, commonly used in the RC community, is to choose a nonlinear activation function such as _f_(_x_) = tanh(_x_) for the nodes and a linear feature vector {{\\mathbb{O}}}\_{{{{{{{\\mathrm{total}}}}}}},i+1}={{\\mathbb{O}}}\_{{{{{{{\\mathrm{lin}}}}}}},i+1}={{{{{{\\bf{r}}}}}}}\_{i+1}\\,in the output layer. The RC is trained using supervised training via regularized least-squares regression. Here, the training data points generate a block of data contained in {{\\mathbb{O}}}\_{{{{{{{\\mathrm{total}}}}}}}} and we match **Y** to the desired output **Y**_d_ in a least-square sense using Tikhonov regularization so that **W**out is given by

{{{{{{\\bf{W}}}}}}}\_{{{{{{{\\mathrm{out}}}}}}}}={{{{{{\\bf{Y}}}}}}}\_{d}{{{\\mathbb{O}}}\_{{{{{{{\\mathrm{total}}}}}}}}}^{T}{\\left({{\\mathbb{O}}}\_{{{{{{{\\mathrm{total}}}}}}}}{{{\\mathbb{O}}}\_{{{{{{{\\mathrm{total}}}}}}}}}^{T}+\\alpha {{{{{\\bf{I}}}}}}\\right)}^{-1},

 (3)

where the regularization parameter _α_, also known as ridge parameter, is set to prevent overfitting to the training data and **I** is the identity matrix.

A different approach to RC is to move the nonlinearity from the reservoir to the output layer[16](https://www.nature.com/articles/s41467-021-25801-2#ref-CR16 "Gonon, L. & Ortega, J. P. Reservoir computing universality with stochastic inputs. IEEE Trans. Neural Netw. Learn. Syst. 31, 100–112 (2020)."),[18](https://www.nature.com/articles/s41467-021-25801-2#ref-CR18 "Bollt, E. On explaining the surprising success of reservoir computing forecaster of chaos? The universal machine learning dynamical system with contrast to VAR and DMD. Chaos 31, 013108 (2021)."). In this case, the reservoir nodes are chosen to have a linear activation function _f_(**r**) = **r**, while the feature vector {{\\mathbb{O}}}\_{{{{{{{\\mathrm{total}}}}}}}} becomes nonlinear. A simple example of such RC is to extend the standard linear feature vector to include the squared values of all nodes, which are obtained through the Hadamard product {{{{{\\bf{r}}}}}}\\odot {{{{{\\bf{r}}}}}}={\\left\[{r}\_{1}^{2},{r}\_{2}^{2},\\ldots ,{r}\_{N}^{2}\\right\]}^{T}[18](https://www.nature.com/articles/s41467-021-25801-2#ref-CR18 "Bollt, E. On explaining the surprising success of reservoir computing forecaster of chaos? The universal machine learning dynamical system with contrast to VAR and DMD. Chaos 31, 013108 (2021)."). Thus, the nonlinear feature vector is given by

{{\\mathbb{O}}}\_{{{{{{{\\mathrm{total}}}}}}}}={{{{{\\bf{r}}}}}}\\oplus \\left({{{{{\\bf{r}}}}}}\\odot {{{{{\\bf{r}}}}}}\\right)={\\left\[{r}\_{1},{r}\_{2},\\ldots ,{r}\_{N},{r}\_{1}^{2},{r}\_{2}^{2},\\ldots ,{r}\_{N}^{2}\\right\]}^{T},

 (4)

where ⊕ represents the vector concatenation operation. A linear reservoir with a nonlinear output is an equivalently powerful universal approximator[16](https://www.nature.com/articles/s41467-021-25801-2#ref-CR16 "Gonon, L. & Ortega, J. P. Reservoir computing universality with stochastic inputs. IEEE Trans. Neural Netw. Learn. Syst. 31, 100–112 (2020).") and shows comparable performance to the standard RC[18](https://www.nature.com/articles/s41467-021-25801-2#ref-CR18 "Bollt, E. On explaining the surprising success of reservoir computing forecaster of chaos? The universal machine learning dynamical system with contrast to VAR and DMD. Chaos 31, 013108 (2021).").

In contrast, the NG-RC creates a feature vector directly from the discretely sample input data with no need for a neural network. Here, {{\\mathbb{O}}}\_{{{{{{{\\mathrm{total}}}}}}}}=c\\ {{\\mathbb{\\oplus }}\\ {\\mathbb{O}}}\_{{{{{{{\\mathrm{lin}}}}}}}}\\oplus {{\\mathbb{O}}}\_{{{{{{{\\mathrm{nonlin}}}}}}}}, where _c_ is a constant and {{\\mathbb{O}}}\_{{{{{{{\\mathrm{nonlin}}}}}}}}\\, is a nonlinear part of the feature vector. Like a traditional RC, the output is obtained using these features in Eq. <a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/articles/s41467-021-25801-2#Equ3">3</a>. We now discuss forming these features.</p><p>The linear features <span class="mathjax-tex"><span class="MathJax\_Preview" style="">{{\\mathbb{O}}}\_{{{{{{{\\mathrm{lin}}}}}}},i}</span><span class="MathJax\_SVG MathJax\_SVG\_Processing" id="MathJax-Element-13-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span><script type="math/tex" id="MathJax-Element-13">{{\\mathbb{O}}}\_{{{{{{{\\mathrm{lin}}}}}}},i} at time step _i_ is composed of observations of the input vector **X** at the current and at _k_\-1 previous times steps spaced by _s_, where (_s_\-1) is the number of skipped steps between consecutive observations. If {{{{{{\\bf{X}}}}}}}\_{i}={\\left\[{x}\_{1,i},{x}\_{2,i},\\ldots ,{x}\_{d,i}\\right\]}^{T} is a _d_\-dimensional vector, {{\\mathbb{O}}}\_{{{{{{{\\mathrm{lin}}}}}}},i} has _d k_ components, and is given by

{{\\mathbb{O}}}\_{{{{{{{\\mathrm{lin}}}}}}},i}={{{{{{\\bf{X}}}}}}}\_{i}\\oplus {{{{{{\\bf{X}}}}}}}\_{i-s}\\oplus {{{{{{\\bf{X}}}}}}}\_{i-2s}\\oplus ...\\oplus {{{{{{\\bf{X}}}}}}}\_{i-\\left(k-1\\right)s}.

 (5)

Based on the general theory of universal approximators[16](https://www.nature.com/articles/s41467-021-25801-2#ref-CR16 "Gonon, L. & Ortega, J. P. Reservoir computing universality with stochastic inputs. IEEE Trans. Neural Netw. Learn. Syst. 31, 100–112 (2020)."),[20](https://www.nature.com/articles/s41467-021-25801-2#ref-CR20 "Franz, M. O. & Schölkopf, B. A unifying view of Wiener and Volterra theory and polynomial kernel regression. Neural. Comput. 18, 3097–3118 (2006)."), _k_ should be taken to be infinitely large. However, it is found in practice that the Volterra series converges rapidly, and hence truncating _k_ to small values does not incur large error. This can also be motivated by considering numerical integration methods of ordinary differential equations where only a few subintervals (steps) in a multistep integrator are needed to obtain high accuracy. We do not subdivide the step size here, but this analogy motivates why small values of _k_ might give good performance in the forecasting tasks considered below.

An important aspect of the NG-RC is that its warm-up period only contains (_sk_) time steps, which are needed to create the feature vector for the first point to be processed. This is a dramatically shorter warm-up period in comparison to traditional RCs, where longer warm-up times are needed to ensure that the reservoir state does not depend on the RC initial conditions. For example, with _s_ \= 1 and _k_ \= 2 as used for some examples below, only two warm-up data points are needed. A typical warm-up time in traditional RC for the same task can be upwards of 103 to 105 data points[12](https://www.nature.com/articles/s41467-021-25801-2#ref-CR12 "Griffith, A., Pomerance, A. & Gauthier, D. J. Forecasting chaotic systems with very low connectivity reservoir computers. Chaos 29, 123108 (2019)."),[14](https://www.nature.com/articles/s41467-021-25801-2#ref-CR14 "Lu, Z., Hunt, B. R. & Ott, E. Attractor reconstruction by machine learning. Chaos 28, 061104 (2018)."). A reduced warm-up time is especially important in situations where it is difficult to obtain data or collecting additional data is too time-consuming.

For the case of a driven dynamical system, {{\\mathbb{O}}}\_{{{{{{{\\mathrm{lin}}}}}}}}(t) also includes the drive signal[21](https://www.nature.com/articles/s41467-021-25801-2#ref-CR21 "Billings, S. A. Nonlinear System Identification (John Wiley & Sons, Ltd., 2013)."). Similarly, a system in which one or more accessible system parameters are adjusted, {{\\mathbb{O}}}\_{{{{{{{\\mathrm{lin}}}}}}}}(t) also includes these parameters[21](https://www.nature.com/articles/s41467-021-25801-2#ref-CR21 "Billings, S. A. Nonlinear System Identification (John Wiley & Sons, Ltd., 2013)."),[22](https://www.nature.com/articles/s41467-021-25801-2#ref-CR22 "Kim, J. Z., Lu, Z., Nozari, E., Papas, G. J. & Bassett, D. S. Teaching recurrent neural networks to infer global temporal structure from local examples. Nat. Mach. Intell. 3, 316–323 (2021).").

The nonlinear part {{\\mathbb{O}}}\_{{{{{{{\\mathrm{nonlin}}}}}}}} of the feature vector is a nonlinear function of {{\\mathbb{O}}}\_{{{{{{{\\mathrm{lin}}}}}}}}. While there is great flexibility in choosing the nonlinear functionals, we find that polynomials provide good prediction ability. Polynomial functionals are the basis of a Volterra representation for dynamical systems[20](https://www.nature.com/articles/s41467-021-25801-2#ref-CR20 "Franz, M. O. & Schölkopf, B. A unifying view of Wiener and Volterra theory and polynomial kernel regression. Neural. Comput. 18, 3097–3118 (2006).") and hence they are a natural starting point. We find that low-order polynomials are enough to obtain high performance.

All monomials of the quadratic polynomial, for example, are captured by the outer product {{\\mathbb{O}}}\_{{{{{{{\\mathrm{lin}}}}}}}}\\otimes {{\\mathbb{O}}}\_{{{{{{{\\mathrm{lin}}}}}}}}, which is a symmetric matrix with (_dk_)2 elements. A quadratic nonlinear feature vector {{\\mathbb{O}}}\_{{{{{{{\\mathrm{nonlinear}}}}}}}}^{(2)}, for example, is composed of the (_dk_) (_dk_+1)⁄2 unique monomials of {{\\mathbb{O}}}\_{{{{{{{\\mathrm{lin}}}}}}}}\\otimes {{\\mathbb{O}}}\_{{{{{{{\\mathrm{lin}}}}}}}}, which are given by the upper triangular elements of the outer product tensor. We define ⌈⊗⌉ as the operator that collects the unique monomials in a vector. Using this notation, a _p_\-order polynomial feature vector is given by

{{\\mathbb{O}}}\_{{{{{{{\\mathrm{nonlinear}}}}}}}}^{(p)}={{\\mathbb{O}}}\_{{{{{{{\\mathrm{lin}}}}}}}}{{\\lceil }}\\otimes {{\\rceil }}{{\\mathbb{O}}}\_{{{{{{{\\mathrm{lin}}}}}}}}{{\\lceil }}\\otimes {{\\rceil }}\\ldots {{\\lceil }}\\otimes {{\\rceil }}{{\\mathbb{O}}}\_{{{{{{{\\mathrm{lin}}}}}}}}

 (6)

with {{\\mathbb{O}}}\_{{{{{{{\\mathrm{lin}}}}}}}} appearing _p_ times.

Recently, it was mathematically proven that the NVAR method is equivalent to a linear RC with polynomial nonlinear readout[18](https://www.nature.com/articles/s41467-021-25801-2#ref-CR18 "Bollt, E. On explaining the surprising success of reservoir computing forecaster of chaos? The universal machine learning dynamical system with contrast to VAR and DMD. Chaos 31, 013108 (2021)."). This means that every NVAR implicitly defines the connectivity matrix and other parameters of a traditional RC described above and that every linear polynomial-readout RC can be expressed as an NVAR. However, the traditional RC is more computationally expensive and requires optimizing many meta-parameters, while the NG-RC is more efficient and straightforward. The NG-RC is doing the same work as the equivalent traditional RC with a full recurrent neural network, but we do not need to find that network explicitly or do any of the costly computation associated with it.

We now introduce models and tasks we use for showcasing the performance of NG-RC. For one of the forecasting tasks and the inference task discussed in the next section, we generate training and testing data by numerically integrating a simplified model of a weather system[23](https://www.nature.com/articles/s41467-021-25801-2#ref-CR23 "Lorenz, E. N. Deterministic nonperiodic flow. J. Atmos. Sci. 20, 130–141 (1963).") developed by Lorenz in 1963\. It consists of a set of three coupled nonlinear differential equations given by

\\dot{x}=10(y-x),\\,\\dot{y}=x(28-z) \\,{-}\\, y,\\,\\dot{z}=xy-8z/3,

 (7)

where the state **X**(_t_) ≡ \[_x_(_t_),_y_(_t_),_z_(_t_)\]_T_ is a vector whose components are Rayleigh–Bénard convection observables. It displays deterministic chaos, sensitive dependence to initial conditions—the so-called butterfly effect—and the phase space trajectory forms a strange attractor shown in Fig. [1](https://www.nature.com/articles/s41467-021-25801-2#Fig1). For future reference, the Lyapunov time for Eq. [7](https://www.nature.com/articles/s41467-021-25801-2#Equ7), which characterizes the divergence timescale for a chaotic system, is 1.1-time units. Below, we refer to this system as Lorenz63.

We also explore using the NG-RC to predict the dynamics of a double-scroll electronic circuit[24](https://www.nature.com/articles/s41467-021-25801-2#ref-CR24 "Chang, A., Bienfang, J. C., Hall, G. M., Gardner, J. R. & Gauthier, D. J. Stabilizing unstable steady states using extended time-delay autosynchronization. Chaos 8, 782–790 (1998).") whose behavior is governed by

{\\dot{V}}\_{1} ={V}\_{1}/{R}\_{1}-\\varDelta V/{R}\_{2}\\,-\\,2{I}\_{r}\\,\\sinh (\\beta \\varDelta V),\\\\ \\dot{{V}\_{2}} =\\varDelta V/{R}\_{2}+2{I}\_{r}\\,\\sinh (\\beta \\varDelta V)-I,\\\\ \\dot{I} ={V}\_{2}-{R}\_{4}I

 (8)

in dimensionless form, where _ΔV_ \= _V_1 – _V_2. Here, we use the parameters _R_1 \= 1.2, _R_2 \= 3.44, _R_4 \= 0.193, _β_ \= 11.6, and _I_ _r_ \= 2.25 × 10−5, which give a Lyapunov time of 7.81-time units.

We select this system because the vector field is not of a polynomial form and _ΔV_ is large enough at some times that a truncated Taylor series expansion of the sinh function gives rise to large differences in the predicted attractor. This task demonstrates that the polynomial form of the feature vector can work for nonpolynomial vector fields as expected from the theory of Volterra representations of dynamical systems[20](https://www.nature.com/articles/s41467-021-25801-2#ref-CR20 "Franz, M. O. & Schölkopf, B. A unifying view of Wiener and Volterra theory and polynomial kernel regression. Neural. Comput. 18, 3097–3118 (2006).").

In the two forecasting tasks presented below, we use an NG-RC to forecast the dynamics of Lorenz63 and the double-scroll system using one-step-ahead prediction. We start with a listening phase, seeking a solution to {{{{{\\bf{X}}}}}}\\left(t+{dt}\\right)={{{{{{\\bf{W}}}}}}}\_{{{{{{{\\mathrm{out}}}}}}}}{{\\mathbb{O}}}\_{{{{{{{\\mathrm{total}}}}}}}}\\left(t\\right), where **W**out is found using Tikhonov regularization[6](https://www.nature.com/articles/s41467-021-25801-2#ref-CR6 "Vogel, C. R. Computational Methods for Inverse Problems (Society for Industrial and Applied Mathematics, 2002)."). During the forecasting (testing) phase, the components of **X**(_t_) are no longer provided to the NG-RC and the predicted output is fed back to the input. Now, the NG-RC is an autonomous dynamical system that predicts the systems’ dynamics if training is successful.

The total feature vector used for the Lorenz63 forecasting task is given by

{{\\mathbb{O}}}\_{{{{{{\\mathrm{total}}}}}}}=c\\oplus {{\\mathbb{O}}}\_{{{{{{\\mathrm{lin}}}}}}}\\oplus {{\\mathbb{O}}}\_{{{{{{\\mathrm{nonlinear}}}}}}}^{(2)},

 (9)

which has \[1+ _d k_+(_d k_) (_d k_+1)/2\] components.

For the double-scroll system forecasting task, we notice that the attractor has odd symmetry and has zero mean for all variables for the parameters we use. To respect these characteristics, we take

{{\\mathbb{O}}}\_{{{{{{\\mathrm{total}}}}}}}={{\\mathbb{O}}}\_{{{{{{\\mathrm{lin}}}}}}}\\oplus {{\\mathbb{O}}}\_{{{{{{\\mathrm{nonlinear}}}}}}}^{(3)}

 (10)

which has \[_d k_+(_d k_) (_d k_+1) (_d k_+2)/6\] components.

For these forecasting tasks, the NG-RC learns simultaneously the vector field and an efficient one-step-ahead integrator to find a mapping from one time to the next without having to learn each separately as in other nonlinear state estimation approaches[25](#ref-CR25 "Crutchfield, J. P. & McNamara, B. S. Equations of motion from a data series. Complex Sys. 1, 417–452 (1987)."),[26](#ref-CR26 "Wang, W.-X., Lai, Y.-C., Grebogi, C. & Ye, J. Network reconstruction based on evolutionary-game data via compressive sensing. Phys. Rev. X 1, 021021 (2011)."),[27](#ref-CR27 "Brunton, S. L., Proctor, J. L., Kutz, J. N. & Bialek, W. Discovering governing equations from data by sparse identification of nonlinear dynamical systems. Proc. Natl Acad. Sci. USA 113, 3932–3937 (2016)."),[28](https://www.nature.com/articles/s41467-021-25801-2#ref-CR28 "Lai, Y.-C. Finding nonlinear system equations and complex network structures from data: a sparse optimization approach. Chaos 31, 082101 (2021)."). The one-step-ahead mapping is known as the flow of the dynamical system and hence the NG-RC learns the flow. To allow the NG-RC to focus on the subtle details of this process, we use a simple Euler-like integration step as a lowest-order approximation to a forecasting step by modifying Eq. [2](https://www.nature.com/articles/s41467-021-25801-2#Equ2) so that the NG-RC learns the difference between the current and future step. To this end, Eq. [2](https://www.nature.com/articles/s41467-021-25801-2#Equ2) is replaced by

{{{{{{\\bf{X}}}}}}}\_{i+1}={{{{{{\\bf{X}}}}}}}\_{i}+{{{{{{\\bf{W}}}}}}}\_{{{{{{{\\mathrm{out}}}}}}}}{{\\mathbb{O}}}\_{{{{{{{\\mathrm{total}}}}}}},i}.

 (11)

In the third task, we provide the NG-RC with all three Lorenz63 variables during training with the goal of inferring the next-step-ahead prediction of one of the variables from the others. During testing, we only provide it with the _x_ and _y_ variables and infer the _z_ variable. This task is important for applications where it is possible to obtain high-quality information about a dynamical variable in a laboratory setting, but not in field deployment. In the field, the observable sensory information is used to infer the missing data.

## Results

For the first task, the ground-truth Lorenz63 strange attractor is shown in Fig. [2a](https://www.nature.com/articles/s41467-021-25801-2#Fig2). The training phase uses only the data shown in Fig. [2b–d](https://www.nature.com/articles/s41467-021-25801-2#Fig2), which consists of 400 data points for each variable with _dt_ \= 0.025, _k_ \= 2, and _s_ \= 1\. The training compute time is <10 ms using Python running on a single-core desktop processor (see Methods). Here, {{\\mathbb{O}}}\_{{{{{{{\\mathrm{total}}}}}}}}\\, has 28 components and **W**out has dimension (3 × 28). The set needs to be long enough for the phase-space trajectory to explore both wings of the strange attractor. The plot is overlayed with the NG-RC predictions during training; no difference is visible on this scale.

**Fig. 2: Forecasting a dynamical system using the NG-RC.**

[![figure 2](https://proxy-prod.omnivore-image-cache.app/685x491,soj55AdGHPwWjIKKbg_uUXM24zjMSdNPH64JuhV1awnA/https://media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41467-021-25801-2/MediaObjects/41467_2021_25801_Fig2_HTML.png)](https://www.nature.com/articles/s41467-021-25801-2/figures/2)

True (**a**) and predicted (**e**) Lorenz63 strange attractors. **b**–**d** Training data set with overlayed predicted behavior with α = 2.5 × 10−6. The normalized root-mean-square error (NRMSE) over one Lyapunov time during the training phase is 1.06 ± 0.01 × 10−4, where the uncertainty is the standard error of the mean. **f**–**h** True (blue) and predicted datasets during the forecasting phase (NRMSE = 2.40 ± 0.53 × 10−3).

The NG-RC is then placed in the prediction phase; a qualitative inspection of the predicted (Fig. [2e](https://www.nature.com/articles/s41467-021-25801-2#Fig2)) and true (Fig. [2a](https://www.nature.com/articles/s41467-021-25801-2#Fig2)) strange attractors shows that they are very similar, indicating that the NG-RC reproduces the long-term climate of Lorenz63 (benchmark problem 2). As seen in Fig. [2f–h](https://www.nature.com/articles/s41467-021-25801-2#Fig2), the NG-RC does a good job of predicting Lorenz63 (benchmark 1), comparable to an optimized traditional RC[3](https://www.nature.com/articles/s41467-021-25801-2#ref-CR3 "Pathak, J., Lu, Z., Hunt, B. R., Girvan, M. & Ott, E. Using machine learning to replicate chaotic attractors and calculate Lyapunov exponents from data. Chaos 27, 121102 (2017)."),[12](https://www.nature.com/articles/s41467-021-25801-2#ref-CR12 "Griffith, A., Pomerance, A. & Gauthier, D. J. Forecasting chaotic systems with very low connectivity reservoir computers. Chaos 29, 123108 (2019)."),[14](https://www.nature.com/articles/s41467-021-25801-2#ref-CR14 "Lu, Z., Hunt, B. R. & Ott, E. Attractor reconstruction by machine learning. Chaos 28, 061104 (2018).") with 100s to 1000s of reservoir nodes. The NG-RC forecasts well out to \~5 Lyapunov times.

In Supplementary Note [1](https://www.nature.com/articles/s41467-021-25801-2#MOESM1), we give other quantitative measurements of the accuracy of the attractor reconstruction and the values of **W**out in Supplementary Note [2](https://www.nature.com/articles/s41467-021-25801-2#MOESM1); there are many components that have substantial weights and that do not appear in the vector field of Eq. [7](https://www.nature.com/articles/s41467-021-25801-2#Equ7), where the vector field is the right-hand-side of the differential equations. This gives quantitative information regarding the difference between the flow and the vector field.

Because the Lyapunov time for the double-scroll system is much longer than for the Lorenz63 system, we extend the training time of the NG-RC from 10 to 100 units to keep the number of Lyapunov times covered during training similar for both cases. To ensure a fair comparison to the Lorenz63 task, we set _dt_ \= 0.25\. With these two changes and the use of the cubic monomials, as given in Eq. [10](https://www.nature.com/articles/s41467-021-25801-2#Equ10), with _d_ \= 3, _k_ \= 2, and _s_ \= 1 for a total of 62 features in {{\\mathbb{O}}}\_{{{{{{{\\mathrm{total}}}}}}}}, the NG-RC uses 400 data points for each variable during training, exactly as in the Lorenz63 task.

Other than these modifications, our method for using the NG-RC to forecast the dynamics of this system proceeds exactly as for the Lorenz63 system. The results of this task are displayed in Fig. [3](https://www.nature.com/articles/s41467-021-25801-2#Fig3), where it is seen that the NG-RC shows similar predictive ability on the double-scroll system as in the Lorenz63 system, where other quantitative measures of accurate attractor reconstruction is given in Supplementary Note [1](https://www.nature.com/articles/s41467-021-25801-2#MOESM1) as well as the components of **W**out in Supplementary Note [2](https://www.nature.com/articles/s41467-021-25801-2#MOESM1).

**Fig. 3: Forecasting the double-scroll system using the NG-RC.**

[![figure 3](https://proxy-prod.omnivore-image-cache.app/685x485,sgWhndq7-lQX8a-rdICjgtN5VW2d2KFHB29LYuRBNQ30/https://media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41467-021-25801-2/MediaObjects/41467_2021_25801_Fig3_HTML.png)](https://www.nature.com/articles/s41467-021-25801-2/figures/3)

True (**a**) and predicted (**e**) double-scroll strange attractors. **b**–**d** Training data set with overlayed predicted behavior. **f**–**h** True (blue) and predicted datasets during the forecasting phase (NRMSE = 4.5 ± 1.0 × 10−3).

In the last task, we infer dynamics not seen by the NG-RC during the testing phase. Here, we use _k_ \= 4 and _s_ \= 5 with _dt_ \= 0.05 to generate an embedding of the full attractor to infer the other component, as informed by Takens’ embedding theorem[29](https://www.nature.com/articles/s41467-021-25801-2#ref-CR29 "Takens, F. Detecting strange attractors in turbulence. In Dynamical Systems and Turbulence, Warwick 1980 (eds Rand, D. & Young, L. S.) 366–381 (Springer, 1981)."). We provide the _x_, _y_, and _z_ variables during training and we again observe that a short training data set of only 400 points is enough to obtain good performance as shown in Fig. [4c](https://www.nature.com/articles/s41467-021-25801-2#Fig4), where the training data set is overlayed with the NG-RC predictions. Here, the total feature vector has 45 components and hence **W**out has dimension (1 × 45). During the testing phase, we only provide the NG-RC with the _x_ and _y_ components (Fig. [4d](https://www.nature.com/articles/s41467-021-25801-2#Fig4), e) and predict the _z_ component (Fig. [4f](https://www.nature.com/articles/s41467-021-25801-2#Fig4)). The performance is nearly identical during the testing phase. The components of **W**out for this task are given in Supplementary Note [2](https://www.nature.com/articles/s41467-021-25801-2#MOESM1).

**Fig. 4: Inference using an NG-RC.**

[![figure 4](https://proxy-prod.omnivore-image-cache.app/685x911,svOQS3xf7plsU0UXsBHMo2NcKW1ZWCN4qt6qrJX8jPl8/https://media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41467-021-25801-2/MediaObjects/41467_2021_25801_Fig4_HTML.png)](https://www.nature.com/articles/s41467-021-25801-2/figures/4)

**a**–**c** Lorenz63 variables during the training phase (blue) and prediction (**c**, red). The predictions overlay the training data in (**c**), resulting in a purple trace (NRMSE = 9.5 ± 0.1 × 10−3 using α = 0.05). **d**–**f** Lorenz63 variables during the testing phase, where the predictions overlay the training data in (**f**), resulting in a purple trace (NRMSE = 1.75 ± 0.3 × 10−2).

## Discussion

The NG-RC is computationally faster than a traditional RC because the feature vector size is much smaller, meaning there are fewer adjustable parameters that must be determined as discussed in [Supplementary Notes](https://www.nature.com/articles/s41467-021-25801-2#MOESM1) [3](https://www.nature.com/articles/s41467-021-25801-2#MOESM1) and [4](https://www.nature.com/articles/s41467-021-25801-2#MOESM1). We believe that the training data set size is reduced precisely because there are fewer fit parameters. Also, as mentioned above, the warmup and training time is shorter, thus reducing the computational time. Finally, the NG-RC has fewer metaparameters to optimize, thus avoiding the computational costly optimization procedure in high-dimensional parameter space. As detailed in Supplementary Note [3](https://www.nature.com/articles/s41467-021-25801-2#MOESM1), we estimate the computational complexity for the Lorenz63 forecasting task and find that the NG-RC is \~33–162 times less costly to simulate than a typical already efficient traditional RC[12](https://www.nature.com/articles/s41467-021-25801-2#ref-CR12 "Griffith, A., Pomerance, A. & Gauthier, D. J. Forecasting chaotic systems with very low connectivity reservoir computers. Chaos 29, 123108 (2019)."), and over 106 times less costly for a high-accuracy traditional RC[14](https://www.nature.com/articles/s41467-021-25801-2#ref-CR14 "Lu, Z., Hunt, B. R. & Ott, E. Attractor reconstruction by machine learning. Chaos 28, 061104 (2018).") for a single set of metaparameters. For the double-scroll system, where the NG-RC has a cubic nonlinearity and hence more features, the improvement is a more modest factor of 8–41 than a typical efficient traditional RC[12](https://www.nature.com/articles/s41467-021-25801-2#ref-CR12 "Griffith, A., Pomerance, A. & Gauthier, D. J. Forecasting chaotic systems with very low connectivity reservoir computers. Chaos 29, 123108 (2019).") for a single set of metaparameters.

The NG-RC builds on previous work on nonlinear system identification. It is most closely related to multi-input, multiple-output nonlinear autoregression with exogenous inputs (NARX) studied since the 1980s[21](https://www.nature.com/articles/s41467-021-25801-2#ref-CR21 "Billings, S. A. Nonlinear System Identification (John Wiley & Sons, Ltd., 2013)."). A crucial distinction is that Tikhonov regularization is not used in the NARX approach and there is no theoretical underpinning of a NARX to an implicit RC. Our NG-RC fuses the best of the NARX methods with modern regression methods, which is needed to obtain the good performance demonstrated here. We mention that Pyle et al.[30](https://www.nature.com/articles/s41467-021-25801-2#ref-CR30 "Pyle, R., Jovanovic, N., Subramanian, D., Palem, K. V. & Patel, A. B. Domain-driven models yield better predictions at lower cost than reservoir computers in Lorenz systems. Philos. Trans. R. Soc. A Math. Phys. Eng. Sci. 379, 24102 (2021).") recently found good performance with a simplified NG-RC but without the theoretical framework and justification presented here.

In other related work, there has been a revival of research on data-driven linearization methods[31](https://www.nature.com/articles/s41467-021-25801-2#ref-CR31 "Carleman, T. Application de la théorie des équations intégrales linéares aux d’équations différentielles non linéares. Acta Math. 59, 63–87 (1932).") that represent the vector field by projecting onto a finite linear subspace spanned by simple functions, usually monomials. Notably, ref. [25](https://www.nature.com/articles/s41467-021-25801-2#ref-CR25 "Crutchfield, J. P. & McNamara, B. S. Equations of motion from a data series. Complex Sys. 1, 417–452 (1987).") uses least-square while recent work uses LASSO[26](https://www.nature.com/articles/s41467-021-25801-2#ref-CR26 "Wang, W.-X., Lai, Y.-C., Grebogi, C. & Ye, J. Network reconstruction based on evolutionary-game data via compressive sensing. Phys. Rev. X 1, 021021 (2011)."),[27](https://www.nature.com/articles/s41467-021-25801-2#ref-CR27 "Brunton, S. L., Proctor, J. L., Kutz, J. N. & Bialek, W. Discovering governing equations from data by sparse identification of nonlinear dynamical systems. Proc. Natl Acad. Sci. USA 113, 3932–3937 (2016).") or information-theoretic methods[32](https://www.nature.com/articles/s41467-021-25801-2#ref-CR32 "Almomani, A. A. R., Sun, J. & Bollt, E. How entropic regression beats the outliers problem in nonlinear system identification. Chaos 30, 013107 (2020).") to simplify the model. The goal of these methods is to model the vector field from data, as opposed to the NG-RC developed here that forecasts over finite time steps and thus learns the flow of the dynamical system. In fact, some of the large-probability components of **W**out (Supplementary Note [2](https://www.nature.com/articles/s41467-021-25801-2#MOESM1)) can be motivated by the terms in the vector field but many others are important, demonstrating that the NG-RC-learned flow is different from the vector field.

Some of the components of **W**out are quite small, suggesting that several features can be removed using various methods without hurting the testing error. In the NARX literature[21](https://www.nature.com/articles/s41467-021-25801-2#ref-CR21 "Billings, S. A. Nonlinear System Identification (John Wiley & Sons, Ltd., 2013)."), it is suggested that a practitioner start with the lowest number of terms in the feature vector and add terms one-by-one, keeping only those terms that reduce substantially the testing error based on an arbitrary cutoff in the observed error reduction. This procedure is tedious and ignores possible correlations in the components. Other theoretically justified approaches include using the LASSO or information-theoretic methods mentioned above. The other approach to reducing the size of the feature space is to use the kernel trick that is the core of ML via support vector machines[20](https://www.nature.com/articles/s41467-021-25801-2#ref-CR20 "Franz, M. O. & Schölkopf, B. A unifying view of Wiener and Volterra theory and polynomial kernel regression. Neural. Comput. 18, 3097–3118 (2006)."). This approach will only give a computational advantage when the dimension of {{\\mathbb{O}}}\_{{{{{{{\\mathrm{total}}}}}}}} is much greater than the number of training data points, which is not the case in our studies here but may be relevant in other situations. We will explore these approaches in future research.

Our study only considers data generated by noise-free numerical simulations of models. It is precisely the use of regularized regression that makes this approach noise-tolerant: it identifies a model that is the best estimator of the underlying dynamics even with noise or uncertainty. We give results for forecasting the Lorenz63 system when it is strongly driven by noise in the Supplementary Note [5](https://www.nature.com/articles/s41467-021-25801-2#MOESM1), where we observe that the NG-RC learns the equivalent noise-free system as long as α is increased demonstrating the importance of regularization.

We also only consider low-dimensional dynamical systems, but previous work forecasting complex high-dimensional spatial-temporal dynamics[4](https://www.nature.com/articles/s41467-021-25801-2#ref-CR4 "Pathak, J., Hunt, B., Girvan, M., Lu, Z. & Ott, E. Model-free prediction of large spatiotemporally chaotic systems from data: a reservoir computing approach. Phys. Rev. Lett. 120, 24102 (2018)."),[7](https://www.nature.com/articles/s41467-021-25801-2#ref-CR7 "Vlachas, P. R. et al. Backpropagation algorithms and reservoir computing in recurrent neural networks for the forecasting of complex spatiotemporal dynamics. Neural Netw. 126, 191–217 (2020).") using a traditional RC suggests that an NG-RC will excel at this task because of the implicit traditional RC but using smaller datasets and requiring optimizing fewer metaparameters. Furthermore, Pyle et al.[30](https://www.nature.com/articles/s41467-021-25801-2#ref-CR30 "Pyle, R., Jovanovic, N., Subramanian, D., Palem, K. V. & Patel, A. B. Domain-driven models yield better predictions at lower cost than reservoir computers in Lorenz systems. Philos. Trans. R. Soc. A Math. Phys. Eng. Sci. 379, 24102 (2021).") successfully forecast the behavior of a multi-scale spatial-temporal system using an approach similar to the NG-RC.

Our work has important implications for learning dynamical systems because there are fewer metaparameters to optimize and the NG-RC only requires extremely short datasets for training. Because the NG-RC has an underlying implicit (hidden) traditional RC, our results generalize to any system for which a standard RC has been applied previously. For example, the NG-RC can be used to create a digital twin for dynamical systems[33](https://www.nature.com/articles/s41467-021-25801-2#ref-CR33 "Grieves, M. W. Virtually Intelligent Product Systems: Digital and Physical Twins. In Complex Systems Engineering: Theory and Practice (eds Flumerfelt, S., et al.) 175–200 (American Institute of Aeronautics and Astronautics, Inc., 2019).") using only observed data or by combining approximate models with observations for data assimilation[34](https://www.nature.com/articles/s41467-021-25801-2#ref-CR34 "Wikner, A. et al. Combining machine learning with knowledge-based modeling for scalable forecasting and subgrid-scale closure of large, complex, spatiotemporal systems. Chaos 30, 053111 (2020)."),[35](https://www.nature.com/articles/s41467-021-25801-2#ref-CR35 "Wikner, A. et al. Using data assimilation to train a hybrid forecast system that combines machine-learning and knowledge-based components. Chaos 31, 053114 (2021)."). It can also be used for nonlinear control of dynamical systems[36](https://www.nature.com/articles/s41467-021-25801-2#ref-CR36 "Canaday, D., Pomerance, A. & Gauthier, D. J. Model-free control of dynamical systems with deep reservoir computing. to appear in J. Phys. Complex. 
                  http://iopscience.iop.org/article/10.1088/2632-072X/ac24f3
                  
                 (2021)."), which can be quickly adjusted to account for changes in the system, or for speeding up the simulation of turbulence[37](https://www.nature.com/articles/s41467-021-25801-2#ref-CR37 "Li, Z. et al. Fourier neural operator for parametric partial differential equations. Preprint at arXiv:2010.08895 (2020). In The International Conference on Learning Representations (ICLR 2021).").

## Methods

The exact numerical results presented here, such as unstable steady states (USSs) and NRMSE, will vary slightly depending on the precise software used to calculate them. We calculate the results for this paper using Python 3.7.9, NumPy 1.20.2, and SciPy 1.6.2 on an x86-64 CPU running Windows 10.

## Data availability

The data generated in this study can be recreated by running the publicly available code as described in the Code availability statement.

## Code availability

All code is available under an MIT License on Github (<https://github.com/quantinfo/ng-rc-paper-code>)[38](https://www.nature.com/articles/s41467-021-25801-2#ref-CR38 "Gauthier, D. J., Griffith, A. & de sa Barbosa, W. ng-rc-paper-code repository. 
                  https://doi.org/10.5281/zenodo.5218954
                  
                 (2021).").

## References

1. Jaeger, H. & Haas, H. Harnessing nonlinearity: predicting chaotic systems and saving energy in wireless communication. _Science_ **304**, 78–80 (2004).  
[Article](https://doi.org/10.1126%2Fscience.1091277) [ADS](http://adsabs.harvard.edu/cgi-bin/nph-data%5Fquery?link%5Ftype=ABSTRACT&bibcode=2004Sci...304...78J) [CAS](https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BD2cXis1ansbc%3D) [ Google Scholar](http://scholar.google.com/scholar%5Flookup?&title=Harnessing%20nonlinearity%3A%20predicting%20chaotic%20systems%20and%20saving%20energy%20in%20wireless%20communication&journal=Science&doi=10.1126%2Fscience.1091277&volume=304&pages=78-80&publication%5Fyear=2004&author=Jaeger%2CH&author=Haas%2CH)
2. Maass, W., Natschläger, T. & Markram, H. Real-time computing without stable states: a new framework for neural computation based on perturbations. _Neural Comput._ **14**, 2531–2560 (2002).  
[Article](https://doi.org/10.1162%2F089976602760407955) [ Google Scholar](http://scholar.google.com/scholar%5Flookup?&title=Real-time%20computing%20without%20stable%20states%3A%20a%20new%20framework%20for%20neural%20computation%20based%20on%20perturbations&journal=Neural%20Comput.&doi=10.1162%2F089976602760407955&volume=14&pages=2531-2560&publication%5Fyear=2002&author=Maass%2CW&author=Natschl%C3%A4ger%2CT&author=Markram%2CH)
3. Pathak, J., Lu, Z., Hunt, B. R., Girvan, M. & Ott, E. Using machine learning to replicate chaotic attractors and calculate Lyapunov exponents from data. _Chaos_ **27**, 121102 (2017).  
[Article](https://doi.org/10.1063%2F1.5010300) [ADS](http://adsabs.harvard.edu/cgi-bin/nph-data%5Fquery?link%5Ftype=ABSTRACT&bibcode=2017Chaos..27l1102P) [MathSciNet](http://www.ams.org/mathscinet-getitem?mr=3732799) [ Google Scholar](http://scholar.google.com/scholar%5Flookup?&title=Using%20machine%20learning%20to%20replicate%20chaotic%20attractors%20and%20calculate%20Lyapunov%20exponents%20from%20data&journal=Chaos&doi=10.1063%2F1.5010300&volume=27&publication%5Fyear=2017&author=Pathak%2CJ&author=Lu%2CZ&author=Hunt%2CBR&author=Girvan%2CM&author=Ott%2CE)
4. Pathak, J., Hunt, B., Girvan, M., Lu, Z. & Ott, E. Model-free prediction of large spatiotemporally chaotic systems from data: a reservoir computing approach. _Phys. Rev. Lett._ **120**, 24102 (2018).  
[Article](https://doi.org/10.1103%2FPhysRevLett.120.024102) [ADS](http://adsabs.harvard.edu/cgi-bin/nph-data%5Fquery?link%5Ftype=ABSTRACT&bibcode=2018PhRvL.120b4102P) [CAS](https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BC1MXltVOqu78%3D) [ Google Scholar](http://scholar.google.com/scholar%5Flookup?&title=Model-free%20prediction%20of%20large%20spatiotemporally%20chaotic%20systems%20from%20data%3A%20a%20reservoir%20computing%20approach&journal=Phys.%20Rev.%20Lett.&doi=10.1103%2FPhysRevLett.120.024102&volume=120&publication%5Fyear=2018&author=Pathak%2CJ&author=Hunt%2CB&author=Girvan%2CM&author=Lu%2CZ&author=Ott%2CE)
5. Bengio, Y., Boulanger-Lewandowski, N. & Pascanu, R. Advances in optimizing recurrent networks. _2013 IEEE International Conference on Acoustics, Speech and Signal Processing_, 2013, pp. 8624–8628 <https://doi.org/10.1109/ICASSP.2013.6639349> (2013).
6. Vogel, C. R. _Computational Methods for Inverse Problems_ (Society for Industrial and Applied Mathematics, 2002).
7. Vlachas, P. R. et al. Backpropagation algorithms and reservoir computing in recurrent neural networks for the forecasting of complex spatiotemporal dynamics. _Neural Netw._ **126**, 191–217 (2020).  
[Article](https://doi.org/10.1016%2Fj.neunet.2020.02.016) [CAS](https://www.nature.com/articles/cas-redirect/1:STN:280:DC%2BB38zhtVSmtw%3D%3D) [ Google Scholar](http://scholar.google.com/scholar%5Flookup?&title=Backpropagation%20algorithms%20and%20reservoir%20computing%20in%20recurrent%20neural%20networks%20for%20the%20forecasting%20of%20complex%20spatiotemporal%20dynamics&journal=Neural%20Netw.&doi=10.1016%2Fj.neunet.2020.02.016&volume=126&pages=191-217&publication%5Fyear=2020&author=Vlachas%2CPR)
8. Bompas, S., Georgeot, B. & Guéry-Odelin, D. Accuracy of neural networks for the simulation of chaotic dynamics: precision of training data vs precision of the algorithm. _Chaos_ **30**, 113118 (2020).  
[Article](https://doi.org/10.1063%2F5.0021264) [ADS](http://adsabs.harvard.edu/cgi-bin/nph-data%5Fquery?link%5Ftype=ABSTRACT&bibcode=2020Chaos..30k3118B) [MathSciNet](http://www.ams.org/mathscinet-getitem?mr=4171102) [CAS](https://www.nature.com/articles/cas-redirect/1:STN:280:DC%2BB3szhvVKqtA%3D%3D) [ Google Scholar](http://scholar.google.com/scholar%5Flookup?&title=Accuracy%20of%20neural%20networks%20for%20the%20simulation%20of%20chaotic%20dynamics%3A%20precision%20of%20training%20data%20vs%20precision%20of%20the%20algorithm&journal=Chaos&doi=10.1063%2F5.0021264&volume=30&publication%5Fyear=2020&author=Bompas%2CS&author=Georgeot%2CB&author=Gu%C3%A9ry-Odelin%2CD)
9. Yperman, J. & Becker, T. Bayesian optimization of hyper-parameters in reservoir computing. Preprint at _arXiv_ _:_ _1611.0519_ (2016).
10. Livi, L., Bianchi, F. M. & Alippi, C. Determination of the edge of criticality in echo state networks through fisher information maximization. _IEEE Trans. Neural Netw. Learn. Syst._ **29**, 706–717 (2018).  
[Article](https://doi.org/10.1109%2FTNNLS.2016.2644268) [MathSciNet](http://www.ams.org/mathscinet-getitem?mr=3774122) [ Google Scholar](http://scholar.google.com/scholar%5Flookup?&title=Determination%20of%20the%20edge%20of%20criticality%20in%20echo%20state%20networks%20through%20fisher%20information%20maximization&journal=IEEE%20Trans.%20Neural%20Netw.%20Learn.%20Syst.&doi=10.1109%2FTNNLS.2016.2644268&volume=29&pages=706-717&publication%5Fyear=2018&author=Livi%2CL&author=Bianchi%2CFM&author=Alippi%2CC)
11. Thiede, L. A. & Parlitz, U. Gradient based hyperparameter optimization in echo state networks. _Neural Netw._ **115**, 23–29 (2019).  
[Article](https://doi.org/10.1016%2Fj.neunet.2019.02.001) [ Google Scholar](http://scholar.google.com/scholar%5Flookup?&title=Gradient%20based%20hyperparameter%20optimization%20in%20echo%20state%20networks&journal=Neural%20Netw.&doi=10.1016%2Fj.neunet.2019.02.001&volume=115&pages=23-29&publication%5Fyear=2019&author=Thiede%2CLA&author=Parlitz%2CU)
12. Griffith, A., Pomerance, A. & Gauthier, D. J. Forecasting chaotic systems with very low connectivity reservoir computers. _Chaos_ **29**, 123108 (2019).  
[Article](https://doi.org/10.1063%2F1.5120710) [ADS](http://adsabs.harvard.edu/cgi-bin/nph-data%5Fquery?link%5Ftype=ABSTRACT&bibcode=2019Chaos..29l3108G) [MathSciNet](http://www.ams.org/mathscinet-getitem?mr=4040519) [ Google Scholar](http://scholar.google.com/scholar%5Flookup?&title=Forecasting%20chaotic%20systems%20with%20very%20low%20connectivity%20reservoir%20computers&journal=Chaos&doi=10.1063%2F1.5120710&volume=29&publication%5Fyear=2019&author=Griffith%2CA&author=Pomerance%2CA&author=Gauthier%2CDJ)
13. Antonik, P., Marsal, N., Brunner, D. & Rontani, D. Bayesian optimisation of large-scale photonic reservoir computers. _Cogn. Comput_. **2021**, 1–9 (2021).  
[ Google Scholar](http://scholar.google.com/scholar%5Flookup?&title=Bayesian%20optimisation%20of%20large-scale%20photonic%20reservoir%20computers&journal=Cogn.%20Comput&volume=2021&pages=1-9&publication%5Fyear=2021&author=Antonik%2CP&author=Marsal%2CN&author=Brunner%2CD&author=Rontani%2CD)
14. Lu, Z., Hunt, B. R. & Ott, E. Attractor reconstruction by machine learning. _Chaos_ **28**, 061104 (2018).  
[Article](https://doi.org/10.1063%2F1.5039508) [ADS](http://adsabs.harvard.edu/cgi-bin/nph-data%5Fquery?link%5Ftype=ABSTRACT&bibcode=2018Chaos..28f1104L) [MathSciNet](http://www.ams.org/mathscinet-getitem?mr=3816936) [ Google Scholar](http://scholar.google.com/scholar%5Flookup?&title=Attractor%20reconstruction%20by%20machine%20learning&journal=Chaos&doi=10.1063%2F1.5039508&volume=28&publication%5Fyear=2018&author=Lu%2CZ&author=Hunt%2CBR&author=Ott%2CE)
15. Platt, J. A., Wong, A. S., Clark, R., Penny, S. G. & Abarbanel, H. D. I. Robust forecasting through generalized synchronization in reservoir computing. Preprint at _arXiv_ _:_ _2103.0036_ (2021).
16. Gonon, L. & Ortega, J. P. Reservoir computing universality with stochastic inputs. _IEEE Trans. Neural Netw. Learn. Syst._ **31**, 100–112 (2020).  
[Article](https://doi.org/10.1109%2FTNNLS.2019.2899649) [MathSciNet](http://www.ams.org/mathscinet-getitem?mr=4056386) [ Google Scholar](http://scholar.google.com/scholar%5Flookup?&title=Reservoir%20computing%20universality%20with%20stochastic%20inputs&journal=IEEE%20Trans.%20Neural%20Netw.%20Learn.%20Syst.&doi=10.1109%2FTNNLS.2019.2899649&volume=31&pages=100-112&publication%5Fyear=2020&author=Gonon%2CL&author=Ortega%2CJP)
17. Hart, A. G., Hook, J. L. & Dawes, J. H. P. Echo state networks trained by Tikhonov least squares are L2(μ) approximators of ergodic dynamical systems. _Phys. D. Nonlinear Phenom._ **421**, 132882 (2021).  
[Article](https://doi.org/10.1016%2Fj.physd.2021.132882) [ Google Scholar](http://scholar.google.com/scholar%5Flookup?&title=Echo%20state%20networks%20trained%20by%20Tikhonov%20least%20squares%20are%20L2%28%CE%BC%29%20approximators%20of%20ergodic%20dynamical%20systems&journal=Phys.%20D.%20Nonlinear%20Phenom.&doi=10.1016%2Fj.physd.2021.132882&volume=421&publication%5Fyear=2021&author=Hart%2CAG&author=Hook%2CJL&author=Dawes%2CJHP)
18. Bollt, E. On explaining the surprising success of reservoir computing forecaster of chaos? The universal machine learning dynamical system with contrast to VAR and DMD. _Chaos_ **31**, 013108 (2021).  
[Article](https://doi.org/10.1063%2F5.0024890) [ADS](http://adsabs.harvard.edu/cgi-bin/nph-data%5Fquery?link%5Ftype=ABSTRACT&bibcode=2021Chaos..31a3108B) [MathSciNet](http://www.ams.org/mathscinet-getitem?mr=4194146) [ Google Scholar](http://scholar.google.com/scholar%5Flookup?&title=On%20explaining%20the%20surprising%20success%20of%20reservoir%20computing%20forecaster%20of%20chaos%3F%20The%20universal%20machine%20learning%20dynamical%20system%20with%20contrast%20to%20VAR%20and%20DMD&journal=Chaos&doi=10.1063%2F5.0024890&volume=31&publication%5Fyear=2021&author=Bollt%2CE)
19. Gauthier, D. J. Reservoir computing: harnessing a universal dynamical system. _SIAM News_ **51**_,_ 12 (2018).
20. Franz, M. O. & Schölkopf, B. A unifying view of Wiener and Volterra theory and polynomial kernel regression. _Neural. Comput._ **18**, 3097–3118 (2006).  
[Article](https://doi.org/10.1162%2Fneco.2006.18.12.3097) [MathSciNet](http://www.ams.org/mathscinet-getitem?mr=2265212) [ Google Scholar](http://scholar.google.com/scholar%5Flookup?&title=A%20unifying%20view%20of%20Wiener%20and%20Volterra%20theory%20and%20polynomial%20kernel%20regression&journal=Neural.%20Comput.&doi=10.1162%2Fneco.2006.18.12.3097&volume=18&pages=3097-3118&publication%5Fyear=2006&author=Franz%2CMO&author=Sch%C3%B6lkopf%2CB)
21. Billings, S. A. _Nonlinear System Identification_ (John Wiley & Sons, Ltd., 2013).
22. Kim, J. Z., Lu, Z., Nozari, E., Papas, G. J. & Bassett, D. S. Teaching recurrent neural networks to infer global temporal structure from local examples. _Nat. Mach. Intell._ **3**, 316–323 (2021).  
[Article](https://doi.org/10.1038%2Fs42256-021-00321-2) [ Google Scholar](http://scholar.google.com/scholar%5Flookup?&title=Teaching%20recurrent%20neural%20networks%20to%20infer%20global%20temporal%20structure%20from%20local%20examples&journal=Nat.%20Mach.%20Intell.&doi=10.1038%2Fs42256-021-00321-2&volume=3&pages=316-323&publication%5Fyear=2021&author=Kim%2CJZ&author=Lu%2CZ&author=Nozari%2CE&author=Papas%2CGJ&author=Bassett%2CDS)
23. Lorenz, E. N. Deterministic nonperiodic flow. _J. Atmos. Sci._ **20**, 130–141 (1963).  
[Article](https://doi.org/10.1175%2F1520-0469%281963%29020%3C0130%3ADNF%3E2.0.CO%3B2) [ADS](http://adsabs.harvard.edu/cgi-bin/nph-data%5Fquery?link%5Ftype=ABSTRACT&bibcode=1963JAtS...20..130L) [MathSciNet](http://www.ams.org/mathscinet-getitem?mr=4021434) [ Google Scholar](http://scholar.google.com/scholar%5Flookup?&title=Deterministic%20nonperiodic%20flow&journal=J.%20Atmos.%20Sci.&doi=10.1175%2F1520-0469%281963%29020%3C0130%3ADNF%3E2.0.CO%3B2&volume=20&pages=130-141&publication%5Fyear=1963&author=Lorenz%2CEN)
24. Chang, A., Bienfang, J. C., Hall, G. M., Gardner, J. R. & Gauthier, D. J. Stabilizing unstable steady states using extended time-delay autosynchronization. _Chaos_ **8**, 782–790 (1998).
25. Crutchfield, J. P. & McNamara, B. S. Equations of motion from a data series. _Complex Sys._ **1**, 417–452 (1987).  
[MathSciNet](http://www.ams.org/mathscinet-getitem?mr=901161) [MATH](http://www.emis.de/MATH-item?0675.58026) [ Google Scholar](http://scholar.google.com/scholar%5Flookup?&title=Equations%20of%20motion%20from%20a%20data%20series&journal=Complex%20Sys.&volume=1&pages=417-452&publication%5Fyear=1987&author=Crutchfield%2CJP&author=McNamara%2CBS)
26. Wang, W.-X., Lai, Y.-C., Grebogi, C. & Ye, J. Network reconstruction based on evolutionary-game data via compressive sensing. _Phys. Rev. X_ **1**, 021021 (2011).  
[ Google Scholar](http://scholar.google.com/scholar%5Flookup?&title=Network%20reconstruction%20based%20on%20evolutionary-game%20data%20via%20compressive%20sensing&journal=Phys.%20Rev.%20X&volume=1&publication%5Fyear=2011&author=Wang%2CW-X&author=Lai%2CY-C&author=Grebogi%2CC&author=Ye%2CJ)
27. Brunton, S. L., Proctor, J. L., Kutz, J. N. & Bialek, W. Discovering governing equations from data by sparse identification of nonlinear dynamical systems. _Proc. Natl Acad. Sci. USA_ **113**, 3932–3937 (2016).  
[Article](https://doi.org/10.1073%2Fpnas.1517384113) [ADS](http://adsabs.harvard.edu/cgi-bin/nph-data%5Fquery?link%5Ftype=ABSTRACT&bibcode=2016PNAS..113.3932B) [MathSciNet](http://www.ams.org/mathscinet-getitem?mr=3494081) [CAS](https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BC28XkvFGis7w%3D) [ Google Scholar](http://scholar.google.com/scholar%5Flookup?&title=Discovering%20governing%20equations%20from%20data%20by%20sparse%20identification%20of%20nonlinear%20dynamical%20systems&journal=Proc.%20Natl%20Acad.%20Sci.%20USA&doi=10.1073%2Fpnas.1517384113&volume=113&pages=3932-3937&publication%5Fyear=2016&author=Brunton%2CSL&author=Proctor%2CJL&author=Kutz%2CJN&author=Bialek%2CW)
28. Lai, Y.-C. Finding nonlinear system equations and complex network structures from data: a sparse optimization approach. Chaos **31**, 082101 (2021).
29. Takens, F. Detecting strange attractors in turbulence. In _Dynamical Systems and Turbulence, Warwick 1980_ (eds Rand, D. & Young, L. S.) 366–381 (Springer, 1981).
30. Pyle, R., Jovanovic, N., Subramanian, D., Palem, K. V. & Patel, A. B. Domain-driven models yield better predictions at lower cost than reservoir computers in Lorenz systems. _Philos. Trans. R. Soc. A Math. Phys. Eng. Sci._ **379**, 24102 (2021).  
[MathSciNet](http://www.ams.org/mathscinet-getitem?mr=4236153) [ Google Scholar](http://scholar.google.com/scholar%5Flookup?&title=Domain-driven%20models%20yield%20better%20predictions%20at%20lower%20cost%20than%20reservoir%20computers%20in%20Lorenz%20systems&journal=Philos.%20Trans.%20R.%20Soc.%20A%20Math.%20Phys.%20Eng.%20Sci.&volume=379&publication%5Fyear=2021&author=Pyle%2CR&author=Jovanovic%2CN&author=Subramanian%2CD&author=Palem%2CKV&author=Patel%2CAB)
31. Carleman, T. Application de la théorie des équations intégrales linéares aux d’équations différentielles non linéares. _Acta Math._ **59**, 63–87 (1932).  
[Article](https://doi.org/10.1007%2FBF02546499) [MathSciNet](http://www.ams.org/mathscinet-getitem?mr=1555355) [ Google Scholar](http://scholar.google.com/scholar%5Flookup?&title=Application%20de%20la%20th%C3%A9orie%20des%20%C3%A9quations%20int%C3%A9grales%20lin%C3%A9ares%20aux%20d%E2%80%99%C3%A9quations%20diff%C3%A9rentielles%20non%20lin%C3%A9ares&journal=Acta%20Math.&doi=10.1007%2FBF02546499&volume=59&pages=63-87&publication%5Fyear=1932&author=Carleman%2CT)
32. Almomani, A. A. R., Sun, J. & Bollt, E. How entropic regression beats the outliers problem in nonlinear system identification. _Chaos_ **30**, 013107 (2020).  
[Article](https://doi.org/10.1063%2F1.5133386) [ADS](http://adsabs.harvard.edu/cgi-bin/nph-data%5Fquery?link%5Ftype=ABSTRACT&bibcode=2020Chaos..30a3107A) [MathSciNet](http://www.ams.org/mathscinet-getitem?mr=4047473) [ Google Scholar](http://scholar.google.com/scholar%5Flookup?&title=How%20entropic%20regression%20beats%20the%20outliers%20problem%20in%20nonlinear%20system%20identification&journal=Chaos&doi=10.1063%2F1.5133386&volume=30&publication%5Fyear=2020&author=Almomani%2CAAR&author=Sun%2CJ&author=Bollt%2CE)
33. Grieves, M. W. Virtually Intelligent Product Systems: Digital and Physical Twins. In _Complex Systems Engineering: Theory and Practice_ (eds Flumerfelt, S., et al.) 175–200 (American Institute of Aeronautics and Astronautics, Inc., 2019).
34. Wikner, A. et al. Combining machine learning with knowledge-based modeling for scalable forecasting and subgrid-scale closure of large, complex, spatiotemporal systems. _Chaos_ **30**, 053111 (2020).  
[Article](https://doi.org/10.1063%2F5.0005541) [ADS](http://adsabs.harvard.edu/cgi-bin/nph-data%5Fquery?link%5Ftype=ABSTRACT&bibcode=2020Chaos..30e3111W) [MathSciNet](http://www.ams.org/mathscinet-getitem?mr=4094693) [ Google Scholar](http://scholar.google.com/scholar%5Flookup?&title=Combining%20machine%20learning%20with%20knowledge-based%20modeling%20for%20scalable%20forecasting%20and%20subgrid-scale%20closure%20of%20large%2C%20complex%2C%20spatiotemporal%20systems&journal=Chaos&doi=10.1063%2F5.0005541&volume=30&publication%5Fyear=2020&author=Wikner%2CA)
35. Wikner, A. et al. Using data assimilation to train a hybrid forecast system that combines machine-learning and knowledge-based components. _Chaos_ **31**, 053114 (2021).  
[Article](https://doi.org/10.1063%2F5.0048050) [ADS](http://adsabs.harvard.edu/cgi-bin/nph-data%5Fquery?link%5Ftype=ABSTRACT&bibcode=2021Chaos..31e3114W) [MathSciNet](http://www.ams.org/mathscinet-getitem?mr=4257785) [ Google Scholar](http://scholar.google.com/scholar%5Flookup?&title=Using%20data%20assimilation%20to%20train%20a%20hybrid%20forecast%20system%20that%20combines%20machine-learning%20and%20knowledge-based%20components&journal=Chaos&doi=10.1063%2F5.0048050&volume=31&publication%5Fyear=2021&author=Wikner%2CA)
36. Canaday, D., Pomerance, A. & Gauthier, D. J. Model-free control of dynamical systems with deep reservoir computing. to appear in _J. Phys. Complex_. <http://iopscience.iop.org/article/10.1088/2632-072X/ac24f3> (2021).
37. Li, Z. et al. Fourier neural operator for parametric partial differential equations. Preprint at _arXiv_ _:_2010.08895 (2020). In The International Conference on Learning Representations (ICLR 2021).
38. Gauthier, D. J., Griffith, A. & de sa Barbosa, W. ng-rc-paper-code repository. <https://doi.org/10.5281/zenodo.5218954> (2021).

[Download references](https://citation-needed.springer.com/v2/references/10.1038/s41467-021-25801-2?format=refman&flavour=references)

## Acknowledgements

We gratefully acknowledge discussions with Henry Abarbanel, Ingo Fischer, and Kathy Lüdge. D.J.G. is supported by the United States Air Force AFRL/SBRK under Contract No. FA864921P0087\. E.B. is supported by the ARO (N68164-EG) and DARPA.

## Author information

### Authors and Affiliations

1. The Ohio State University, Department of Physics, 191 West Woodruff Ave., Columbus, OH, 43210, USA  
Daniel J. Gauthier, Aaron Griffith & Wendson A. S. Barbosa
2. ResCon Technologies, LLC, PO Box 21229, Columbus, OH, 43221, USA  
Daniel J. Gauthier
3. Clarkson University, Department of Electrical and Computer Engineering, Potsdam, NY, 13669, USA  
Erik Bollt
4. Clarkson Center for Complex Systems Science (C3S2), Potsdam, NY, 13699, USA  
Erik Bollt

Authors

1. Daniel J. Gauthier  
You can also search for this author in[PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&term=Daniel%20J.%20Gauthier) [Google Scholar](http://scholar.google.co.uk/scholar?as%5Fq=&num=10&btnG=Search+Scholar&as%5Fepq=&as%5Foq=&as%5Feq=&as%5Focct=any&as%5Fsauthors=%22Daniel%20J.%20Gauthier%22&as%5Fpublication=&as%5Fylo=&as%5Fyhi=&as%5Fallsubj=all&hl=en)
2. Erik Bollt  
You can also search for this author in[PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&term=Erik%20Bollt) [Google Scholar](http://scholar.google.co.uk/scholar?as%5Fq=&num=10&btnG=Search+Scholar&as%5Fepq=&as%5Foq=&as%5Feq=&as%5Focct=any&as%5Fsauthors=%22Erik%20Bollt%22&as%5Fpublication=&as%5Fylo=&as%5Fyhi=&as%5Fallsubj=all&hl=en)
3. Aaron Griffith  
You can also search for this author in[PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&term=Aaron%20Griffith) [Google Scholar](http://scholar.google.co.uk/scholar?as%5Fq=&num=10&btnG=Search+Scholar&as%5Fepq=&as%5Foq=&as%5Feq=&as%5Focct=any&as%5Fsauthors=%22Aaron%20Griffith%22&as%5Fpublication=&as%5Fylo=&as%5Fyhi=&as%5Fallsubj=all&hl=en)
4. Wendson A. S. Barbosa  
You can also search for this author in[PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&term=Wendson%20A.%20S.%20Barbosa) [Google Scholar](http://scholar.google.co.uk/scholar?as%5Fq=&num=10&btnG=Search+Scholar&as%5Fepq=&as%5Foq=&as%5Feq=&as%5Focct=any&as%5Fsauthors=%22Wendson%20A.%20S.%20Barbosa%22&as%5Fpublication=&as%5Fylo=&as%5Fyhi=&as%5Fallsubj=all&hl=en)

### Contributions

D.J.G. optimized the NG-RC, performed the simulations in the main text, and drafted the manuscript. E.B. conceptualized the connection between an RC and NVAR, helped interpret the data and edited the manuscript. A.G. and W.A.S.B. helped interpret the data and edited the manuscript.

### Corresponding author

Correspondence to[Daniel J. Gauthier](mailto:gauthier.51@osu.edu).

## Ethics declarations

### Competing interests

D.J.G. has financial interests as a cofounder of ResCon Technologies, LCC, which is commercializing RCs. The remaining authors declare no competing interests.

## Additional information

**Peer review information** _Nature Communications_ thanks Serhiy Yanchuk and the other anonymous reviewer(s) for their contribution to the peer review of this work. Peer reviewer reports are available.

**Publisher’s note** Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.

## Supplementary information

## Rights and permissions

**Open Access** This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. The images or other third party material in this article are included in the article’s Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the article’s Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this license, visit <http://creativecommons.org/licenses/by/4.0/>.

[Reprints and Permissions](https://s100.copyright.com/AppDispatchServlet?title=Next%20generation%20reservoir%20computing&author=Daniel%20J.%20Gauthier%20et%20al&contentID=10.1038%2Fs41467-021-25801-2©right=The%20Author%28s%29&publication=2041-1723&publicationDate=2021-09-21&publisherName=SpringerNature&orderBeanReset=true&oa=CC%20BY)

## About this article

[![Check for updates. Verify currency and authenticity via CrossMark](https://proxy-prod.omnivore-image-cache.app/57x81,sEiG9LKi069qnp2ryl-KOdoy0ai17ndMEK4wRAPZmTqM/data:image/svg+xml;base64,<svg height="81" width="57" xmlns="http://www.w3.org/2000/svg"><g fill="none" fill-rule="evenodd"><path d="m17.35 35.45 21.3-14.2v-17.03h-21.3" fill="#989898"/><path d="m38.65 35.45-21.3-14.2v-17.03h21.3" fill="#747474"/><path d="m28 .5c-12.98 0-23.5 10.52-23.5 23.5s10.52 23.5 23.5 23.5 23.5-10.52 23.5-23.5c0-6.23-2.48-12.21-6.88-16.62-4.41-4.4-10.39-6.88-16.62-6.88zm0 41.25c-9.8 0-17.75-7.95-17.75-17.75s7.95-17.75 17.75-17.75 17.75 7.95 17.75 17.75c0 4.71-1.87 9.22-5.2 12.55s-7.84 5.2-12.55 5.2z" fill="#535353"/><path d="m41 36c-5.81 6.23-15.23 7.45-22.43 2.9-7.21-4.55-10.16-13.57-7.03-21.5l-4.92-3.11c-4.95 10.7-1.19 23.42 8.78 29.71 9.97 6.3 23.07 4.22 30.6-4.86z" fill="#9c9c9c"/><path d="m.2 58.45c0-.75.11-1.42.33-2.01s.52-1.09.91-1.5c.38-.41.83-.73 1.34-.94.51-.22 1.06-.32 1.65-.32.56 0 1.06.11 1.51.35.44.23.81.5 1.1.81l-.91 1.01c-.24-.24-.49-.42-.75-.56-.27-.13-.58-.2-.93-.2-.39 0-.73.08-1.05.23-.31.16-.58.37-.81.66-.23.28-.41.63-.53 1.04-.13.41-.19.88-.19 1.39 0 1.04.23 1.86.68 2.46.45.59 1.06.88 1.84.88.41 0 .77-.07 1.07-.23s.59-.39.85-.68l.91 1c-.38.43-.8.76-1.28.99-.47.22-1 .34-1.58.34-.59 0-1.13-.1-1.64-.31-.5-.2-.94-.51-1.31-.91-.38-.4-.67-.9-.88-1.48-.22-.59-.33-1.26-.33-2.02zm8.4-5.33h1.61v2.54l-.05 1.33c.29-.27.61-.51.96-.72s.76-.31 1.24-.31c.73 0 1.27.23 1.61.71.33.47.5 1.14.5 2.02v4.31h-1.61v-4.1c0-.57-.08-.97-.25-1.21-.17-.23-.45-.35-.83-.35-.3 0-.56.08-.79.22-.23.15-.49.36-.78.64v4.8h-1.61zm7.37 6.45c0-.56.09-1.06.26-1.51.18-.45.42-.83.71-1.14.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.36c.07.62.29 1.1.65 1.44.36.33.82.5 1.38.5.29 0 .57-.04.83-.13s.51-.21.76-.37l.55 1.01c-.33.21-.69.39-1.09.53-.41.14-.83.21-1.26.21-.48 0-.92-.08-1.34-.25-.41-.16-.76-.4-1.07-.7-.31-.31-.55-.69-.72-1.13-.18-.44-.26-.95-.26-1.52zm4.6-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.07.45-.31.29-.5.73-.58 1.3zm2.5.62c0-.57.09-1.08.28-1.53.18-.44.43-.82.75-1.13s.69-.54 1.1-.71c.42-.16.85-.24 1.31-.24.45 0 .84.08 1.17.23s.61.34.85.57l-.77 1.02c-.19-.16-.38-.28-.56-.37-.19-.09-.39-.14-.61-.14-.56 0-1.01.21-1.35.63-.35.41-.52.97-.52 1.67 0 .69.17 1.24.51 1.66.34.41.78.62 1.32.62.28 0 .54-.06.78-.17.24-.12.45-.26.64-.42l.67 1.03c-.33.29-.69.51-1.08.65-.39.15-.78.23-1.18.23-.46 0-.9-.08-1.31-.24-.4-.16-.75-.39-1.05-.7s-.53-.69-.7-1.13c-.17-.45-.25-.96-.25-1.53zm6.91-6.45h1.58v6.17h.05l2.54-3.16h1.77l-2.35 2.8 2.59 4.07h-1.75l-1.77-2.98-1.08 1.23v1.75h-1.58zm13.69 1.27c-.25-.11-.5-.17-.75-.17-.58 0-.87.39-.87 1.16v.75h1.34v1.27h-1.34v5.6h-1.61v-5.6h-.92v-1.2l.92-.07v-.72c0-.35.04-.68.13-.98.08-.31.21-.57.4-.79s.42-.39.71-.51c.28-.12.63-.18 1.04-.18.24 0 .48.02.69.07.22.05.41.1.57.17zm.48 5.18c0-.57.09-1.08.27-1.53.17-.44.41-.82.72-1.13.3-.31.65-.54 1.04-.71.39-.16.8-.24 1.23-.24s.84.08 1.24.24c.4.17.74.4 1.04.71s.54.69.72 1.13c.19.45.28.96.28 1.53s-.09 1.08-.28 1.53c-.18.44-.42.82-.72 1.13s-.64.54-1.04.7-.81.24-1.24.24-.84-.08-1.23-.24-.74-.39-1.04-.7c-.31-.31-.55-.69-.72-1.13-.18-.45-.27-.96-.27-1.53zm1.65 0c0 .69.14 1.24.43 1.66.28.41.68.62 1.18.62.51 0 .9-.21 1.19-.62.29-.42.44-.97.44-1.66 0-.7-.15-1.26-.44-1.67-.29-.42-.68-.63-1.19-.63-.5 0-.9.21-1.18.63-.29.41-.43.97-.43 1.67zm6.48-3.44h1.33l.12 1.21h.05c.24-.44.54-.79.88-1.02.35-.24.7-.36 1.07-.36.32 0 .59.05.78.14l-.28 1.4-.33-.09c-.11-.01-.23-.02-.38-.02-.27 0-.56.1-.86.31s-.55.58-.77 1.1v4.2h-1.61zm-47.87 15h1.61v4.1c0 .57.08.97.25 1.2.17.24.44.35.81.35.3 0 .57-.07.8-.22.22-.15.47-.39.73-.73v-4.7h1.61v6.87h-1.32l-.12-1.01h-.04c-.3.36-.63.64-.98.86-.35.21-.76.32-1.24.32-.73 0-1.27-.24-1.61-.71-.33-.47-.5-1.14-.5-2.02zm9.46 7.43v2.16h-1.61v-9.59h1.33l.12.72h.05c.29-.24.61-.45.97-.63.35-.17.72-.26 1.1-.26.43 0 .81.08 1.15.24.33.17.61.4.84.71.24.31.41.68.53 1.11.13.42.19.91.19 1.44 0 .59-.09 1.11-.25 1.57-.16.47-.38.85-.65 1.16-.27.32-.58.56-.94.73-.35.16-.72.25-1.1.25-.3 0-.6-.07-.9-.2s-.59-.31-.87-.56zm0-2.3c.26.22.5.37.73.45.24.09.46.13.66.13.46 0 .84-.2 1.15-.6.31-.39.46-.98.46-1.77 0-.69-.12-1.22-.35-1.61-.23-.38-.61-.57-1.13-.57-.49 0-.99.26-1.52.77zm5.87-1.69c0-.56.08-1.06.25-1.51.16-.45.37-.83.65-1.14.27-.3.58-.54.93-.71s.71-.25 1.08-.25c.39 0 .73.07 1 .2.27.14.54.32.81.55l-.06-1.1v-2.49h1.61v9.88h-1.33l-.11-.74h-.06c-.25.25-.54.46-.88.64-.33.18-.69.27-1.06.27-.87 0-1.56-.32-2.07-.95s-.76-1.51-.76-2.65zm1.67-.01c0 .74.13 1.31.4 1.7.26.38.65.58 1.15.58.51 0 .99-.26 1.44-.77v-3.21c-.24-.21-.48-.36-.7-.45-.23-.08-.46-.12-.7-.12-.45 0-.82.19-1.13.59-.31.39-.46.95-.46 1.68zm6.35 1.59c0-.73.32-1.3.97-1.71.64-.4 1.67-.68 3.08-.84 0-.17-.02-.34-.07-.51-.05-.16-.12-.3-.22-.43s-.22-.22-.38-.3c-.15-.06-.34-.1-.58-.1-.34 0-.68.07-1 .2s-.63.29-.93.47l-.59-1.08c.39-.24.81-.45 1.28-.63.47-.17.99-.26 1.54-.26.86 0 1.51.25 1.93.76s.63 1.25.63 2.21v4.07h-1.32l-.12-.76h-.05c-.3.27-.63.48-.98.66s-.73.27-1.14.27c-.61 0-1.1-.19-1.48-.56-.38-.36-.57-.85-.57-1.46zm1.57-.12c0 .3.09.53.27.67.19.14.42.21.71.21.28 0 .54-.07.77-.2s.48-.31.73-.56v-1.54c-.47.06-.86.13-1.18.23-.31.09-.57.19-.76.31s-.33.25-.41.4c-.09.15-.13.31-.13.48zm6.29-3.63h-.98v-1.2l1.06-.07.2-1.88h1.34v1.88h1.75v1.27h-1.75v3.28c0 .8.32 1.2.97 1.2.12 0 .24-.01.37-.04.12-.03.24-.07.34-.11l.28 1.19c-.19.06-.4.12-.64.17-.23.05-.49.08-.76.08-.4 0-.74-.06-1.02-.18-.27-.13-.49-.3-.67-.52-.17-.21-.3-.48-.37-.78-.08-.3-.12-.64-.12-1.01zm4.36 2.17c0-.56.09-1.06.27-1.51s.41-.83.71-1.14c.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.37c.08.62.29 1.1.65 1.44.36.33.82.5 1.38.5.3 0 .58-.04.84-.13.25-.09.51-.21.76-.37l.54 1.01c-.32.21-.69.39-1.09.53s-.82.21-1.26.21c-.47 0-.92-.08-1.33-.25-.41-.16-.77-.4-1.08-.7-.3-.31-.54-.69-.72-1.13-.17-.44-.26-.95-.26-1.52zm4.61-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.08.45-.31.29-.5.73-.57 1.3zm3.01 2.23c.31.24.61.43.92.57.3.13.63.2.98.2.38 0 .65-.08.83-.23s.27-.35.27-.6c0-.14-.05-.26-.13-.37-.08-.1-.2-.2-.34-.28-.14-.09-.29-.16-.47-.23l-.53-.22c-.23-.09-.46-.18-.69-.3-.23-.11-.44-.24-.62-.4s-.33-.35-.45-.55c-.12-.21-.18-.46-.18-.75 0-.61.23-1.1.68-1.49.44-.38 1.06-.57 1.83-.57.48 0 .91.08 1.29.25s.71.36.99.57l-.74.98c-.24-.17-.49-.32-.73-.42-.25-.11-.51-.16-.78-.16-.35 0-.6.07-.76.21-.17.15-.25.33-.25.54 0 .14.04.26.12.36s.18.18.31.26c.14.07.29.14.46.21l.54.19c.23.09.47.18.7.29s.44.24.64.4c.19.16.34.35.46.58.11.23.17.5.17.82 0 .3-.06.58-.17.83-.12.26-.29.48-.51.68-.23.19-.51.34-.84.45-.34.11-.72.17-1.15.17-.48 0-.95-.09-1.41-.27-.46-.19-.86-.41-1.2-.68z" fill="#535353"/></g></svg>)](https://crossmark.crossref.org/dialog/?doi=10.1038/s41467-021-25801-2)

### Cite this article

Gauthier, D.J., Bollt, E., Griffith, A. _et al._ Next generation reservoir computing._Nat Commun_ **12**, 5564 (2021). https://doi.org/10.1038/s41467-021-25801-2

[Download citation](https://citation-needed.springer.com/v2/references/10.1038/s41467-021-25801-2?format=refman&flavour=citation)

* Received:
* Accepted:
* Published:
* DOI: https://doi.org/10.1038/s41467-021-25801-2

### Subjects

---

