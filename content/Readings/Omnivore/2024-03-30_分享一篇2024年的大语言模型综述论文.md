---
id: e5313f12-4d87-43dd-b0e7-82db2cd980f8
title: |
  分享一篇2024年的大语言模型综述论文
author: |
  人工智能大讲堂
date_saved: 2024-03-30 01:56:57
date_published: 2024-03-26 16:46:00
draft: true
---

# 分享一篇2024年的大语言模型综述论文
#Omnivore

[Read on Omnivore](https://omnivore.app/me/https-mp-weixin-qq-com-s-j-qt-1-p-hegx-bjf-hu-tv-dgl-x-og-18e8dee9cb9)

[Read Original](https://mp.weixin.qq.com/s/JQt1PHegxBjfHuTVDglXOg)

date_saved: 2024-03-30 01:56:57

date_published: 2024-03-26 16:46:00

--- 

# Full Content: 

![cover_image](https://proxy-prod.omnivore-image-cache.app/0x0,sDXziMgcmPTFy3-Bq7HALDcEXQCGg2co0FZEM8TtWlr4/https://mmbiz.qpic.cn/sz_mmbiz_jpg/gWS53OdTR9TZWDYoKhzg1ujWibicdFuw6FLjFLlmtZZCj9mibjqHmDiaZF8aAvn6z8aXeyv2ZfcGTsYmdTHtfbxp0A/0?wx_fmt=jpeg) 

原创  人工智能大讲堂  人工智能大讲堂 _2024-03-27 04:46_ _辽宁_ 

今天给大家分享一篇有关大模型的综述性论文，Github获得8k小星星，发布时间是2024年，所以内容涵盖了有关大模型的最新进展。

项目网址：  

https://github.com/RUCAIBox/LLMSurvey

**或者后台回复llms获取pdf，论文有中文版。** 

以下是文档内容的思维导图概括：

`- 大型语言模型综述
  - 背景
    - 语言的理解与表达
    - 统计语言模型到神经语言模型的发展
    - Transformer模型的兴起
  - 主要发现
    - 预训练语言模型(PLM)的兴起
    - LLM的涌现能力
    - 规模法则与性能关系
  - 技术
    - 预训练任务
    - 适配微调
    - 使用策略
    - 能力评估
  - 可用资源
    - 开源代码库
    - 公开模型检查点
    - 语料库资源
  - 问题与挑战
    - 涌现能力的未解之谜
    - 训练与微调的高成本
    - 潜在的安全隐患
  - 发展方向
    - 理论基础研究
    - 模型架构优化
    - 训练效率提升
    - 安全性与对齐性增强
  - 附录
    - 相关论文与链接
    - 作者信息与致谢`

结合文档内容，我再多说几句。

**语言模型**  

大模型的本质是构建一个语言模型来对自然语言建模。

最常见的NLP任务是生成文本，例如，已知几个词，通过语言模型去预测下一个词，GPT系列就是通过自回归方式生成下一个词；或者给出前后几个词，预测中间的词，Bert就是这种类似完形填空的模型。无论是自回归还是完形填空，都被称为基于上下文的文本生成。  

语言模型通过预测词典中每个词出现在这个位置的概率来生成文本。

构建语言建模经历了从**统计语言模型**到现在的**神经网络模型**。

统计语言模型：马尔科夫链

神经网络模型：RNN，LSTM（ELMO），Transformer（GPT，Bert，T5，LLAMA）。

**大语言模型的涌现能力**  

在计算机视觉领域，通过增加卷积层个数能增加模型提取特征的能力，但随着层数的增加，模型性能增加会逐渐放缓。  

但在NLP中，Tranformer打破了CV中的Scale定律，通过扩展模型的大小或者数据量，模型的性能会一直增加下去。且模型超过某个体量后就会出现涌现现象。  

涌现现象通常表现在上下文理解能力，指令遵循，逐步推理。

**大模型关键技术**

预训练：并行训练，分布式训练。

能力引导：使用有监督数据对预训练模型进行微调，使其适应下游任务。  

对齐微调：通过强化学习，使其向人类对齐。

扩展：但随着模型的扩展，计算资源消耗也会急剧增长。

---

